{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of different Parameters (Features: Section Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:01:26.583952Z",
     "start_time": "2019-01-10T15:01:10.437028Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pivottablejs import pivot_ui\n",
    "\n",
    "# my package\n",
    "from packageMeinhart import PhysioDataHandler as PDH\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_precision_recall_accuracy\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_misclassified_data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the class *PhysioData_SectionFeatures* for feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:01:26.647955Z",
     "start_time": "2019-01-10T15:01:26.595952Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PhysioData_SectionFeatures in module packageMeinhart.PhysioDataHandler:\n",
      "\n",
      "class PhysioData_SectionFeatures(builtins.object)\n",
      " |  Class for feature generation using section means.\n",
      " |  There are various selectable options --> see Parameters. \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  num_sections : int\n",
      " |      Number of equally partitioned sections to split the single repetitions of the signals.\n",
      " |      \n",
      " |  test_subject_ids : int or list (of int)\n",
      " |      Subject IDs to select for testing (e.g. [1, 2, 3]).\n",
      " |      --> default -1: Select all subjects.\n",
      " |      --> if test_subject_ids is an empty list: empty DataFrame is returned by corresponding method.\n",
      " |      \n",
      " |  train_subject_ids : int or list\n",
      " |      Subject IDs to select for training (e.g. [1, 2, 3]).\n",
      " |      --> default -1: Select all subjects not in test_subject_ids.\n",
      " |      --> if train_subject_ids is an empty list: empty DataFrame is returned by corresponding method.\n",
      " |      \n",
      " |  test_rep_nums : int or list\n",
      " |      Repetition numbers to select for testing (e.g. [5, 10]).\n",
      " |      --> default -1: Select all repetitions.\n",
      " |      \n",
      " |  train_rep_nums : int or list\n",
      " |      Repetition numbers to select for training (e.g. [5, 10]).\n",
      " |      --> default -1: Select all repetitions.\n",
      " |      \n",
      " |  test_ex_abbrs : int or list\n",
      " |      Exercise abbreviations to select for testing (e.g. ['RF', 'SA']).\n",
      " |      --> default -1: Select all exercise abbreviations.\n",
      " |      \n",
      " |  train_ex_abbrs : int or list\n",
      " |      Exercise abbreviations to select for training (e.g. ['RF', 'SA']).\n",
      " |      --> default -1: Select all exercise abbreviations.\n",
      " |  \n",
      " |  with_non_Ex : boolean\n",
      " |      If False --> omit non exercise data (data points with zero repetitions).\n",
      " |      \n",
      " |  rot_axis_test_data : int\n",
      " |      Axis for rotation:\n",
      " |      0, 1 or 2 --> x, y or z\n",
      " |  \n",
      " |  rot_angle_test_data : int or float\n",
      " |      Rotation angle in degree.\n",
      " |  \n",
      " |  add_noise_test_data : boolean\n",
      " |      If True --> Additive White Gaussian Noise (AWGN) is added to signals of data for testing.\n",
      " |  \n",
      " |  snr_db : int or float\n",
      " |      Desired signal to noise ratio in db for the generated noisy test signals.\n",
      " |  \n",
      " |  csv_data_dir : string\n",
      " |      Directory of signal data csv-files.\n",
      " |      \n",
      " |  csv_skiprows : int\n",
      " |      Number of rows to skip for signal data csv-files.\n",
      " |      \n",
      " |  csv_separator : string\n",
      " |      Separator for signal data csv-files.\n",
      " |  \n",
      " |  data_base_path : string\n",
      " |      Path to data base (containing at least the following):\n",
      " |          - subject IDs\n",
      " |          - exercise abbreviations\n",
      " |          - number of repetitions\n",
      " |          - sequence numbers\n",
      " |          - start times\n",
      " |          - stop times\n",
      " |          - csv-file name\n",
      " |      \n",
      " |  print_progress : boolean\n",
      " |      If True --> print progress at feature generation.\n",
      " |  \n",
      " |  signal_abbrs : list of strings\n",
      " |      Abbreviations of the signals (e.g. ['Acc','Gyr']).\n",
      " |  \n",
      " |  signal_orientations : list of strings\n",
      " |      Orientations of the signals (e.g. ['x','y','z']).\n",
      " |      \n",
      " |  labels_abbr2num_dict : dict\n",
      " |      Dictionary to convert exercise abbreviations to number (e.g. ={'RF':0,'RO':1,'RS':2, ... }).\n",
      " |  \n",
      " |  sub_id_key : string\n",
      " |      Key of the DataFrame for subject IDs.\n",
      " |      \n",
      " |  num_rep_key : string\n",
      " |      Key of the DataFrame for repetition numbers.\n",
      " |      \n",
      " |  abbreviation_key : string\n",
      " |      Key of the DataFrame for exercise abbreviations.\n",
      " |      \n",
      " |  start_time_key : strings\n",
      " |      Start time key for DataFrame which contains data base entries.\n",
      " |      \n",
      " |  stop_time_key : strings\n",
      " |      Stop time key for DataFrame which contains data base entries.\n",
      " |      \n",
      " |  csv_file_key : strings\n",
      " |      csv-file key for DataFrame which contains data base entries.\n",
      " |      \n",
      " |  sampling_rate : int or float\n",
      " |      Sampling rate of the signals in Hz.\n",
      " |      \n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  X_test_df : DataFrame\n",
      " |      Features for testing.\n",
      " |  \n",
      " |  y_test_df : DataFrame\n",
      " |      Labels for testing.\n",
      " |  \n",
      " |  X_train_df : DataFrame\n",
      " |      Features for training.\n",
      " |  \n",
      " |  y_train_df : DataFrame\n",
      " |      Labels for testing.\n",
      " |  \n",
      " |  \n",
      " |  test_data_points_df : DataFrame\n",
      " |      Data points for testing from data base.\n",
      " |  \n",
      " |  train_data_points_df : DataFrame\n",
      " |      Data points for training from data base.\n",
      " |  \n",
      " |  all_data_points_df : DataFrame\n",
      " |      All data points from data base.\n",
      " |  \n",
      " |  \n",
      " |  Methods\n",
      " |  -------\n",
      " |  get_X_test_df()\n",
      " |      Returns features for testing as DataFrame.\n",
      " |  \n",
      " |  get_y_test_df() :\n",
      " |      Returns labels for testing as DataFrame.\n",
      " |  \n",
      " |  get_X_train_df() :\n",
      " |      Returns features for training as DataFrame.\n",
      " |  \n",
      " |  get_y_train_df() :\n",
      " |      Returns labels for testing as DataFrame.\n",
      " |  \n",
      " |  \n",
      " |  X_test():\n",
      " |      Returns feature matrix for testing as np.array.\n",
      " |  \n",
      " |  y_test():\n",
      " |      Returns numeric labels for testing as np.array.\n",
      " |  \n",
      " |  X_train():\n",
      " |      Returns feature matrix for training as np.array.\n",
      " |  \n",
      " |  y_train():\n",
      " |      Returns numeric labels for training as np.array.\n",
      " |  \n",
      " |  \n",
      " |  get_test_data_points()\n",
      " |      Returns data points for testing from data base as DataFrame.\n",
      " |  \n",
      " |  get_train_data_points()\n",
      " |      Returns data points for training from data base as DataFrame.\n",
      " |  \n",
      " |  get_all_data_points()\n",
      " |      Returns all data points from data base as DataFrame.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  X_test(self)\n",
      " |      # methods to get feature values only\n",
      " |  \n",
      " |  X_train(self)\n",
      " |  \n",
      " |  __init__(self, num_sections=10, test_subject_ids=-1, train_subject_ids=-1, test_rep_nums=-1, train_rep_nums=-1, test_ex_abbrs=-1, train_ex_abbrs=-1, with_non_Ex=True, rot_axis_test_data=0, rot_angle_test_data=0, add_noise_test_data=False, add_noise_train_data=False, snr_db=20, csv_data_dir='E:\\\\Physio_Data_Split_Ex_and_NonEx', csv_skiprows=0, csv_separator=',', data_base_path='E:\\\\Physio_Data\\\\DataBase_Physio_with_nonEx.db', print_progress=True, signal_abbrs=['Acc', 'Gyr'], signal_orientations=['x', 'y', 'z'], labels_abbr2num_dict={'RF': 0, 'RO': 1, 'RS': 2, 'LR': 3, 'BC': 4, 'TC': 5, 'MP': 6, 'SA': 7, 'P1': 8, 'P2': 9, 'NE': 10}, sub_id_key='subject_id', num_rep_key='num_rep', abbreviation_key='abbreviation', start_time_key='start_time', stop_time_key='stop_time', csv_file_key='csv_file', sampling_rate=256)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      --> See class docstring.\n",
      " |  \n",
      " |  get_X_test_df(self)\n",
      " |      # methods to get features\n",
      " |  \n",
      " |  get_X_train_df(self)\n",
      " |  \n",
      " |  get_all_data_points(self)\n",
      " |  \n",
      " |  get_test_data_points(self)\n",
      " |      # methods to get data points (DataFrames)\n",
      " |  \n",
      " |  get_train_data_points(self)\n",
      " |  \n",
      " |  get_y_test_df(self)\n",
      " |  \n",
      " |  get_y_train_df(self)\n",
      " |  \n",
      " |  y_test(self)\n",
      " |  \n",
      " |  y_train(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PDH.PhysioData_SectionFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instance of physio data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:02:31.519666Z",
     "start_time": "2019-01-10T15:01:26.660956Z"
    }
   },
   "outputs": [],
   "source": [
    "PD1 = PDH.PhysioData_SectionFeatures(num_sections=10,\n",
    "                                     test_subject_ids=1,\n",
    "                                     train_subject_ids=-1,\n",
    "                                     test_rep_nums=-1,\n",
    "                                     train_rep_nums=-1,\n",
    "                                     test_ex_abbrs=-1,\n",
    "                                     train_ex_abbrs=-1,\n",
    "                                     with_non_Ex=True,\n",
    "                                     rot_axis_test_data=0,\n",
    "                                     rot_angle_test_data=0,\n",
    "                                     add_noise_test_data=False,\n",
    "                                     add_noise_train_data=False,\n",
    "                                     snr_db=20,\n",
    "                                     csv_data_dir='E:\\Physio_Data_Split_Ex_and_NonEx',\n",
    "                                     csv_skiprows=0,\n",
    "                                     csv_separator=',',\n",
    "                                     data_base_path='E:\\Physio_Data\\DataBase_Physio_with_nonEx.db',\n",
    "                                     print_progress=True,\n",
    "                                     signal_abbrs=['Acc','Gyr'],\n",
    "                                     signal_orientations=['x','y','z'],\n",
    "                                     labels_abbr2num_dict={'RF':0,'RO':1,'RS':2,'LR':3,'BC':4,'TC':5,\n",
    "                                                           'MP':6,'SA':7,'P1':8,'P2':9,'NE':10},\n",
    "                                     sub_id_key='subject_id',\n",
    "                                     num_rep_key='num_rep',\n",
    "                                     abbreviation_key='abbreviation',\n",
    "                                     start_time_key='start_time',\n",
    "                                     stop_time_key='stop_time',\n",
    "                                     csv_file_key='csv_file',\n",
    "                                     sampling_rate=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting selected data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:02:31.553668Z",
     "start_time": "2019-01-10T15:02:31.529666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"PD1_test.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xfc266d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_ui(PD1.get_test_data_points(), \n",
    "         rows=['abbreviation'], \n",
    "         cols=['subject_id', 'num_rep'], \n",
    "         outfile_path=\"PD1_test.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting selected data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:02:31.820683Z",
     "start_time": "2019-01-10T15:02:31.563668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"PD1_train.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1bc0f278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_ui(PD1.get_train_data_points(), \n",
    "         rows=['abbreviation'], \n",
    "         cols=['subject_id', 'num_rep'], \n",
    "         outfile_path=\"PD1_train.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (ML part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:02:36.892973Z",
     "start_time": "2019-01-10T15:02:31.830683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# StandardScaler raises the following warning:\n",
    "# --> DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
    "# if we want to ignore that:\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it with different ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:06:48.664282Z",
     "start_time": "2019-01-10T16:06:47.845235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "\n",
      "Total Accuracy: 98.87%\n",
      "\n",
      "Exercise\tPrecision [%]\tRecall [%]\tAccuracy[%]\n",
      "  RF\t\t  100.00\t   93.33\t   99.72\n",
      "  RO\t\t   93.75\t  100.00\t   99.72\n",
      "  RS\t\t  100.00\t  100.00\t  100.00\n",
      "  LR\t\t  100.00\t  100.00\t  100.00\n",
      "  BC\t\t  100.00\t  100.00\t  100.00\n",
      "  TC\t\t  100.00\t  100.00\t  100.00\n",
      "  MP\t\t   85.71\t  100.00\t   99.29\n",
      "  SA\t\t  100.00\t  100.00\t  100.00\n",
      "  P1\t\t  100.00\t   96.67\t   99.86\n",
      "  P2\t\t  100.00\t  100.00\t  100.00\n",
      "  NE\t\t   99.75\t   98.77\t   99.15\n",
      "\n",
      "8 misclassified (709 test data points):\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "P1 classified as NE\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n"
     ]
    }
   ],
   "source": [
    "# create ML model\n",
    "ML_model = RandomForestClassifier(n_estimators=50, max_leaf_nodes=40, n_jobs=-1, random_state=42)\n",
    "#ML_model = make_pipeline(StandardScaler(), SVC(random_state=42)) # Support Vector Classifier with input scaling\n",
    "\n",
    "# train the model\n",
    "ML_model.fit(PD1.X_train(), PD1.y_train())\n",
    "\n",
    "# predict labels\n",
    "y_pred = ML_model.predict(PD1.X_test())\n",
    "\n",
    "# show results\n",
    "print('Model: ' + type(ML_model).__name__ + '\\n')\n",
    "print('Total Accuracy: {:.2f}%\\n'.format((accuracy_score(PD1.y_test(), y_pred))*100))\n",
    "print_precision_recall_accuracy(y_pred, PD1.y_test())\n",
    "print('')\n",
    "print_misclassified_data_points(y_pred, PD1.y_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:02:37.790024Z",
     "start_time": "2019-01-10T15:02:37.761023Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_elements = [('scale', StandardScaler()), ('clf', SVC())]\n",
    "#pipe_elements = [('scale', StandardScaler()), ('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(pipe_elements)\n",
    "\n",
    "param_grid = {'clf__C': [0.1, 1, 10],\n",
    "              'clf__gamma': ['scale', 0.01, 0.1]}\n",
    "\n",
    "# include PCA\n",
    "#param_grid = {'reduce_dim__n_components': [10, 20, 30],\n",
    "#              'clf__C': [1, 10, 100],\n",
    "#              'clf__gamma': [1, 10, 100]}\n",
    "\n",
    "# C: Penalty parameter C of the error term. (Regularisation parameter)\n",
    "# gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. (Bandwidth of kernel)\n",
    "\n",
    "# splitting strategy for grid search: stratified CV with 5 folds\n",
    "grid_search = GridSearchCV(pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', \n",
    "                           verbose=10, \n",
    "                           n_jobs=-1, \n",
    "                           return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.182080Z",
     "start_time": "2019-01-10T15:02:37.794025Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:  1.4min remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'clf__C': [0.1, 1, 10], 'clf__gamma': ['scale', 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply grid search and cross validation\n",
    "grid_search.fit(PD1.X_train(), PD1.y_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.277086Z",
     "start_time": "2019-01-10T15:04:06.188080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.73%\n"
     ]
    }
   ],
   "source": [
    "# show score (test data)\n",
    "print('Accuracy: {:.2f}%'.format(grid_search.score(PD1.X_test(), PD1.y_test())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.370091Z",
     "start_time": "2019-01-10T15:04:06.281086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__C': 1, 'clf__gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "# show best parameters\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.545101Z",
     "start_time": "2019-01-10T15:04:06.374091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score at cross validatoin: 98.95%\n"
     ]
    }
   ],
   "source": [
    "# show best score of cross validation\n",
    "print('Best score at cross validatoin: {:.2f}%'.format(grid_search.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.689109Z",
     "start_time": "2019-01-10T15:04:06.558102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n"
     ]
    }
   ],
   "source": [
    "# show best estimator\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:06.895121Z",
     "start_time": "2019-01-10T15:04:06.700110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.188325</td>\n",
       "      <td>0.488628</td>\n",
       "      <td>0.977288</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__gamma': 'scale'}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978013</td>\n",
       "      <td>0.981603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982829</td>\n",
       "      <td>0.978763</td>\n",
       "      <td>0.971358</td>\n",
       "      <td>0.976929</td>\n",
       "      <td>0.985246</td>\n",
       "      <td>0.978776</td>\n",
       "      <td>0.255620</td>\n",
       "      <td>0.133182</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.775702</td>\n",
       "      <td>0.561232</td>\n",
       "      <td>0.955392</td>\n",
       "      <td>0.957676</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__gamma': 0.01}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.945440</td>\n",
       "      <td>0.956051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953393</td>\n",
       "      <td>0.960588</td>\n",
       "      <td>0.943535</td>\n",
       "      <td>0.947121</td>\n",
       "      <td>0.986885</td>\n",
       "      <td>0.975714</td>\n",
       "      <td>0.145710</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>0.010245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.603607</td>\n",
       "      <td>0.652237</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.934069</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__gamma': 0.1}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.909609</td>\n",
       "      <td>0.933974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.930176</td>\n",
       "      <td>0.876230</td>\n",
       "      <td>0.936939</td>\n",
       "      <td>0.517072</td>\n",
       "      <td>0.055035</td>\n",
       "      <td>0.015592</td>\n",
       "      <td>0.002675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788645</td>\n",
       "      <td>0.173610</td>\n",
       "      <td>0.989542</td>\n",
       "      <td>0.991748</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'clf__C': 1, 'clf__gamma': 'scale'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985342</td>\n",
       "      <td>0.992845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995094</td>\n",
       "      <td>0.990811</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>0.991833</td>\n",
       "      <td>0.988525</td>\n",
       "      <td>0.992449</td>\n",
       "      <td>0.139816</td>\n",
       "      <td>0.010984</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.000834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.554832</td>\n",
       "      <td>0.184010</td>\n",
       "      <td>0.987908</td>\n",
       "      <td>0.990849</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'clf__C': 1, 'clf__gamma': 0.01}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.991619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994276</td>\n",
       "      <td>0.990402</td>\n",
       "      <td>0.985270</td>\n",
       "      <td>0.991017</td>\n",
       "      <td>0.988525</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.109707</td>\n",
       "      <td>0.041625</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.000892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.212413</td>\n",
       "      <td>0.506629</td>\n",
       "      <td>0.965523</td>\n",
       "      <td>0.995180</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'clf__C': 1, 'clf__gamma': 0.1}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.967427</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973017</td>\n",
       "      <td>0.994895</td>\n",
       "      <td>0.970540</td>\n",
       "      <td>0.994079</td>\n",
       "      <td>0.945082</td>\n",
       "      <td>0.995306</td>\n",
       "      <td>2.265312</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.028259</td>\n",
       "      <td>0.205612</td>\n",
       "      <td>0.987092</td>\n",
       "      <td>0.994567</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'clf__C': 10, 'clf__gamma': 'scale'}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.986971</td>\n",
       "      <td>0.995707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988553</td>\n",
       "      <td>0.994282</td>\n",
       "      <td>0.987725</td>\n",
       "      <td>0.993671</td>\n",
       "      <td>0.979508</td>\n",
       "      <td>0.995306</td>\n",
       "      <td>0.278948</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.575833</td>\n",
       "      <td>0.118807</td>\n",
       "      <td>0.988072</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'clf__C': 10, 'clf__gamma': 0.01}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.988599</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987735</td>\n",
       "      <td>0.992649</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>0.993467</td>\n",
       "      <td>0.982787</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.093972</td>\n",
       "      <td>0.008424</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.260044</td>\n",
       "      <td>0.456826</td>\n",
       "      <td>0.961765</td>\n",
       "      <td>0.999591</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'clf__C': 10, 'clf__gamma': 0.1}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.963355</td>\n",
       "      <td>0.999387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938525</td>\n",
       "      <td>0.999184</td>\n",
       "      <td>2.280580</td>\n",
       "      <td>0.136060</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       2.188325         0.488628         0.977288          0.979167   \n",
       "1       1.775702         0.561232         0.955392          0.957676   \n",
       "2      10.603607         0.652237         0.901961          0.934069   \n",
       "3       0.788645         0.173610         0.989542          0.991748   \n",
       "4       0.554832         0.184010         0.987908          0.990849   \n",
       "5      14.212413         0.506629         0.965523          0.995180   \n",
       "6       1.028259         0.205612         0.987092          0.994567   \n",
       "7       0.575833         0.118807         0.988072          0.993750   \n",
       "8      11.260044         0.456826         0.961765          0.999591   \n",
       "\n",
       "  param_clf__C param_clf__gamma                                  params  \\\n",
       "0          0.1            scale  {'clf__C': 0.1, 'clf__gamma': 'scale'}   \n",
       "1          0.1             0.01     {'clf__C': 0.1, 'clf__gamma': 0.01}   \n",
       "2          0.1              0.1      {'clf__C': 0.1, 'clf__gamma': 0.1}   \n",
       "3            1            scale    {'clf__C': 1, 'clf__gamma': 'scale'}   \n",
       "4            1             0.01       {'clf__C': 1, 'clf__gamma': 0.01}   \n",
       "5            1              0.1        {'clf__C': 1, 'clf__gamma': 0.1}   \n",
       "6           10            scale   {'clf__C': 10, 'clf__gamma': 'scale'}   \n",
       "7           10             0.01      {'clf__C': 10, 'clf__gamma': 0.01}   \n",
       "8           10              0.1       {'clf__C': 10, 'clf__gamma': 0.1}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score       ...         \\\n",
       "0                5           0.978013            0.981603       ...          \n",
       "1                8           0.945440            0.956051       ...          \n",
       "2                9           0.909609            0.933974       ...          \n",
       "3                1           0.985342            0.992845       ...          \n",
       "4                3           0.984528            0.991619       ...          \n",
       "5                6           0.967427            0.996934       ...          \n",
       "6                4           0.986971            0.995707       ...          \n",
       "7                2           0.988599            0.994890       ...          \n",
       "8                7           0.963355            0.999387       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0           0.982829            0.978763           0.971358   \n",
       "1           0.953393            0.960588           0.943535   \n",
       "2           0.921504            0.932203           0.893617   \n",
       "3           0.995094            0.990811           0.988543   \n",
       "4           0.994276            0.990402           0.985270   \n",
       "5           0.973017            0.994895           0.970540   \n",
       "6           0.988553            0.994282           0.987725   \n",
       "7           0.987735            0.992649           0.988543   \n",
       "8           0.967294            1.000000           0.968903   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0            0.976929           0.985246            0.978776      0.255620   \n",
       "1            0.947121           0.986885            0.975714      0.145710   \n",
       "2            0.930176           0.876230            0.936939      0.517072   \n",
       "3            0.991833           0.988525            0.992449      0.139816   \n",
       "4            0.991017           0.988525            0.991837      0.109707   \n",
       "5            0.994079           0.945082            0.995306      2.265312   \n",
       "6            0.993671           0.979508            0.995306      0.278948   \n",
       "7            0.993467           0.982787            0.994898      0.093972   \n",
       "8            1.000000           0.938525            0.999184      2.280580   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.133182        0.006289         0.001524  \n",
       "1        0.123523        0.016059         0.010245  \n",
       "2        0.055035        0.015592         0.002675  \n",
       "3        0.010984        0.003193         0.000834  \n",
       "4        0.041625        0.003470         0.000892  \n",
       "5        0.061838        0.010362         0.000962  \n",
       "6        0.048626        0.004266         0.000802  \n",
       "7        0.008424        0.003150         0.000972  \n",
       "8        0.136060        0.011846         0.000342  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and evaluate directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:47.516444Z",
     "start_time": "2019-01-10T15:04:06.899121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "\n",
      "Total Accuracy: 98.60%\n",
      "\n",
      "Exercise\tPrecision [%]\tRecall [%]\tAccuracy[%]\n",
      "  RF\t\t   71.43\t  100.00\t   99.57\n",
      "  RO\t\t  100.00\t   60.00\t   99.57\n",
      "  RS\t\t  100.00\t  100.00\t  100.00\n",
      "  LR\t\t  100.00\t  100.00\t  100.00\n",
      "  BC\t\t  100.00\t  100.00\t  100.00\n",
      "  TC\t\t  100.00\t  100.00\t  100.00\n",
      "  MP\t\t   52.63\t  100.00\t   99.03\n",
      "  SA\t\t  100.00\t  100.00\t  100.00\n",
      "  P1\t\t  100.00\t  100.00\t  100.00\n",
      "  P2\t\t  100.00\t  100.00\t  100.00\n",
      "  NE\t\t  100.00\t   98.91\t   99.03\n",
      "\n",
      "13 misclassified (926 test data points):\n",
      "RO classified as RF\n",
      "RO classified as RF\n",
      "RO classified as RF\n",
      "RO classified as RF\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n"
     ]
    }
   ],
   "source": [
    "PD2 = PDH.PhysioData_SectionFeatures(num_sections=10,\n",
    "                                     test_subject_ids=[1,2],\n",
    "                                     train_subject_ids=-1,\n",
    "                                     test_rep_nums=5,\n",
    "                                     train_rep_nums=[10, 15],\n",
    "                                     test_ex_abbrs=-1,\n",
    "                                     train_ex_abbrs=-1,\n",
    "                                     with_non_Ex=True,\n",
    "                                     rot_axis_test_data=0,\n",
    "                                     rot_angle_test_data=10,\n",
    "                                     add_noise_test_data=True,\n",
    "                                     add_noise_train_data=False,\n",
    "                                     snr_db=20)\n",
    "\n",
    "# create ML model\n",
    "ML_model = RandomForestClassifier(n_estimators=50, max_leaf_nodes=40, n_jobs=-1, random_state=42)\n",
    "#ML_model = make_pipeline(StandardScaler(), SVC(random_state=42)) # Support Vector Classifier with input scaling\n",
    "\n",
    "# train the model\n",
    "ML_model.fit(PD2.X_train(), PD2.y_train())\n",
    "\n",
    "# predict labels\n",
    "y_pred = ML_model.predict(PD2.X_test())\n",
    "\n",
    "# show results\n",
    "print('Model: ' + type(ML_model).__name__ + '\\n')\n",
    "print('Total Accuracy: {:.2f}%\\n'.format((accuracy_score(PD2.y_test(), y_pred))*100))\n",
    "print_precision_recall_accuracy(y_pred, PD2.y_test())\n",
    "print('')\n",
    "print_misclassified_data_points(y_pred, PD2.y_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:47.532445Z",
     "start_time": "2019-01-10T15:04:47.522445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:47.701455Z",
     "start_time": "2019-01-10T15:04:47.541446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scale', StandardScaler(copy=True, with_mean=True, with_std=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:47.925468Z",
     "start_time": "2019-01-10T15:04:47.708455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['clf'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T15:04:48.043474Z",
     "start_time": "2019-01-10T15:04:47.932468Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## set number of principal components\n",
    "#number_principal_comp = 30\n",
    "#\n",
    "## make pca model\n",
    "#pca = PCA(n_components=number_principal_comp)\n",
    "#\n",
    "## create new features from PCA projections\n",
    "#X_train_pca = pca.fit_transform(X_train_for_pca)\n",
    "#X_test_pca = pca.transform(X_test_for_pca)\n",
    "#\n",
    "#\n",
    "## make LDA model\n",
    "#lda = LDA()\n",
    "## create new features from LDA projections\n",
    "#X_train_lda = lda.fit_transform(X_train_for_lda, y_train_lda)\n",
    "#X_test_lda = lda.transform(X_test_for_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
