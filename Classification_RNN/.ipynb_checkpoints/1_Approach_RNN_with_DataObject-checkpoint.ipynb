{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:37.137693Z",
     "start_time": "2019-01-07T10:13:34.194524Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pivottablejs import pivot_ui\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') # in order to import modules from my own package\n",
    "from packageMeinhart import functionsMasterProjectMeinhart as fmpm\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_precision_recall_accuracy\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_misclassified_data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to handle data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:37.157694Z",
     "start_time": "2019-01-07T10:13:37.140693Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_database(data_base_path='DataBase_Physio_with_nonEx.db'):\n",
    "    '''\n",
    "    Function to load the following data from data base:\n",
    "        - subject IDs\n",
    "        - exercise abbreviations\n",
    "        - number of repetitions\n",
    "        - sequence numbers\n",
    "        - start times\n",
    "        - stop times\n",
    "        - csv-file name\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_base_path : string\n",
    "        Path to data base.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame with the listet information (see above).\n",
    "    '''\n",
    "    # Connect to an existing database\n",
    "    conn = sqlite3.connect(data_base_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # sql command to extract data\n",
    "    query_sql = \"\"\"\n",
    "        SELECT e.subject_id,\n",
    "        p.abbreviation,\n",
    "        e.num_rep,\n",
    "        r.sequence_num,\n",
    "        r.start_time, r.stop_time,\n",
    "        e.csv_file\n",
    "        FROM subjects s\n",
    "        INNER JOIN exercises e\n",
    "        ON s.id = e.subject_id\n",
    "        INNER JOIN paradigms p\n",
    "        ON p.id = e.paradigm_id\n",
    "        INNER JOIN repetitions r\n",
    "        ON e.id = r.exercise_id\n",
    "        \"\"\"\n",
    "    \n",
    "    # get data from data base and close connection\n",
    "    all_data_points_df = pd.read_sql_query(query_sql, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return all_data_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:37.365706Z",
     "start_time": "2019-01-07T10:13:37.164694Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_data_points_from_df(all_data_points_df,\n",
    "                               subject_ids=-1,\n",
    "                               subject_ids_complementary=[],\n",
    "                               reps=-1,\n",
    "                               abbrs=-1,\n",
    "                               with_non_Ex=True,\n",
    "                               sub_id_key='subject_id',\n",
    "                               num_rep_key='num_rep',\n",
    "                               abbreviation_key='abbreviation'):\n",
    "    '''\n",
    "    Function to select data points from a DataFrame based on subject IDs,\n",
    "    number of repetitions and exercise abbreviations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_data_points_df : pandas DataFrame\n",
    "        DataFrame with all data points.\n",
    "    \n",
    "    subject_ids : int or list\n",
    "        Subject IDs to select (e.g. [1, 2, 3]).\n",
    "        --> default -1: Select all subjects.\n",
    "        \n",
    "    subject_ids_complementary : int or list\n",
    "        If subject_ids is -1 --> select only subjects not in subject_ids_complementary.\n",
    "        \n",
    "    reps : int or list\n",
    "        Repetition numbers to select (e.g. [5, 10]).\n",
    "        --> default -1: Select all repetitions.\n",
    "        \n",
    "    abbrs : int or list\n",
    "        Exercise abbreviations to select (e.g. ['RF', 'SA']).\n",
    "        --> default -1: Select all exercise abbreviations.\n",
    "    \n",
    "    with_non_Ex : boolean\n",
    "        If False --> omit non exercise data (data points with zero repetitions).\n",
    "        \n",
    "    sub_id_key : string\n",
    "        Key of the DataFrame for subject IDs.\n",
    "        \n",
    "    num_rep_key : string\n",
    "        Key of the DataFrame for repetition numbers.\n",
    "        \n",
    "    abbreviation_key : string\n",
    "        Key of the DataFrame for exercise abbreviations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame with selected data points.\n",
    "    '''\n",
    "    \n",
    "    data_points_df = all_data_points_df.copy()\n",
    "    \n",
    "    # select the subject IDs\n",
    "    if subject_ids is -1 and subject_ids_complementary:\n",
    "        if not isinstance(subject_ids_complementary, list): # if not list --> make list\n",
    "            subject_ids_complementary = [subject_ids_complementary]\n",
    "        data_points_df = data_points_df.loc[~data_points_df[sub_id_key].isin(subject_ids_complementary)]\n",
    "        \n",
    "    elif subject_ids is not -1:\n",
    "        if not isinstance(subject_ids, list): # if not list --> make list\n",
    "            subject_ids = [subject_ids]\n",
    "        data_points_df = data_points_df.loc[data_points_df[sub_id_key].isin(subject_ids)]\n",
    "\n",
    "    # select the repetition numbers\n",
    "    if reps is not -1:\n",
    "        if not isinstance(reps, list): # if not list --> make list\n",
    "            reps = [reps]\n",
    "        if with_non_Ex is True:\n",
    "            reps.append(0) # zero repetitions correspond to non exercise data\n",
    "        data_points_df = data_points_df.loc[data_points_df[num_rep_key].isin(reps)]\n",
    "\n",
    "    elif with_non_Ex is False:\n",
    "        data_points_df = data_points_df.loc[data_points_df[num_rep_key] != 0]\n",
    "        \n",
    "    # select the exercise abbreviations\n",
    "    if abbrs is not -1:\n",
    "        if not isinstance(abbrs, list): # if not list --> make list\n",
    "            abbrs = [abbrs]\n",
    "        data_points_df = data_points_df.loc[data_points_df[abbreviation_key].isin(abbrs)]\n",
    "\n",
    "    return data_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:37.513714Z",
     "start_time": "2019-01-07T10:13:37.369706Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_progress_func(current_num, max_num, prev_prog, add_info=None):\n",
    "    '''\n",
    "    Function to print progress [%] in a loop.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_num : int\n",
    "        Number of the current run in a loop.\n",
    "        \n",
    "    max_num : int\n",
    "        Maximum number of runs in a loop.\n",
    "        \n",
    "    prev_prog : int\n",
    "        Previous progress, to print only if necessary.\n",
    "        \n",
    "    add_info : str\n",
    "        Additional information to print instead of \"Progress\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Previous progress, important for next run.\n",
    "    '''\n",
    "    new_prog = int(current_num/max_num*100)\n",
    "    \n",
    "    if new_prog > prev_prog:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if isinstance(add_info, str):\n",
    "            print(add_info + ' {:3d}%'.format(new_prog))\n",
    "        else:\n",
    "            print('Progress: {:3d}%'.format(new_prog))\n",
    "        \n",
    "    return new_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:37.984741Z",
     "start_time": "2019-01-07T10:13:37.516714Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequences_from_separate_repetitions(data_points_df,\n",
    "                   max_sequence_length=6,\n",
    "                   orig_sampling_rate=256,\n",
    "                   new_sampling_rate=8,\n",
    "                   cutoff=10,\n",
    "                   order=6,\n",
    "                   csv_data_dir='E:\\Physio_Data_Split_Ex_and_NonEx',\n",
    "                   csv_skiprows=0,\n",
    "                   csv_separator=',',\n",
    "                   signal_abbrs=['Acc','Gyr'],\n",
    "                   signal_orientations=['x','y','z'],\n",
    "                   labels_abbr2num_dict={'RF':0,'RO':1,'RS':2,'LR':3,'BC':4,'TC':5,'MP':6,'SA':7,'P1':8,'P2':9,'NE':10},\n",
    "                   abbreviation_key='abbreviation',\n",
    "                   start_time_key='start_time',\n",
    "                   stop_time_key='stop_time',\n",
    "                   csv_file_key='csv_file',\n",
    "                   print_progress=True,\n",
    "                   progress_info='Generate sequences...'):\n",
    "    '''\n",
    "    Function to generate sequences from separate repetitions, changing the sampling rate\n",
    "    and saving them to a tensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_points_df : DataFrame\n",
    "        DataFrame with information about data points (see load_data_from_database()).\n",
    "        \n",
    "    max_sequence_length : int or float\n",
    "        Maximum sequence length to consider in seconds.\n",
    "        \n",
    "    orig_sampling_rate : int or float\n",
    "        Original sampling rate of the signals in Hz.\n",
    "        \n",
    "    new_sampling_rate : int or float\n",
    "        New sampling rate of the signals in Hz.\n",
    "        \n",
    "    cutoff : int or float\n",
    "        Cutoff frequency for filtering.\n",
    "    \n",
    "    order : int\n",
    "        Order of butterworth filter.\n",
    "        \n",
    "    csv_data_dir : string\n",
    "        Directory of signal data csv-files.\n",
    "        \n",
    "    csv_skiprows : int\n",
    "        Number of rows to skip for signal data csv-files.\n",
    "        \n",
    "    csv_separator : string\n",
    "        Separator for signal data csv-files.\n",
    "        \n",
    "    signal_abbrs : list of strings\n",
    "        Abbreviations of the signals (e.g. ['Acc','Gyr']).\n",
    "    \n",
    "    signal_orientations : list of strings\n",
    "        Orientations of the signals (e.g. ['x','y','z']).\n",
    "        \n",
    "    labels_abbr2num_dict : dict\n",
    "        Dictionary to convert exercise abbreviations to number (e.g. ={'RF':0,'RO':1,'RS':2, ... }).\n",
    "    \n",
    "    abbreviation_key : strings\n",
    "        Exercise abbreviation key for DataFrame which contains data base entries.\n",
    "        \n",
    "    start_time_key : strings\n",
    "        Start time key for DataFrame which contains data base entries.\n",
    "        \n",
    "    stop_time_key : strings\n",
    "        Stop time key for DataFrame which contains data base entries.\n",
    "        \n",
    "    csv_file_key : strings\n",
    "        csv-file key for DataFrame which contains data base entries.\n",
    "        \n",
    "    print_progress : boolean\n",
    "        If True --> print progress at signal sequences generation.\n",
    "        \n",
    "    progress_info : strings\n",
    "        Additional information to print with progress.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_all, y_all, seqlens_all\n",
    "        \n",
    "        X_all ... tensor with signal sequences (dimensions: [number of data points, \n",
    "                                                             max sequence length, \n",
    "                                                             number of signals])\n",
    "        y_all ... array with labels\n",
    "        seqlens_all ... array with sequence lengths\n",
    "    '''\n",
    "    \n",
    "    # max index of new sequences\n",
    "    max_sequ_index = max_sequence_length * new_sampling_rate\n",
    "    \n",
    "    # number of signals (Acc: x, y, z; Gyr: x, y, z --> 6 signals)\n",
    "    num_signals = len(signal_abbrs) * len(signal_orientations)\n",
    "    \n",
    "    # number to exercise-abbreviations dict\n",
    "    labels_num2abbr_dict = {num: abbr for abbr, num in labels_abbr2num_dict.items()}\n",
    "                                                   \n",
    "    # create array for labels\n",
    "    y_all = np.zeros(len(data_points_df), dtype=np.int8)\n",
    "    \n",
    "    # create tensor for sequences\n",
    "    X_all = np.zeros((len(data_points_df), max_sequ_index, num_signals))\n",
    "    \n",
    "    # create matrix for sequence lengths\n",
    "    seqlens_all = np.zeros(len(data_points_df), dtype=np.int)\n",
    "    \n",
    "    # sampling rate ratio of original and new sampling rate (e.g. if ratio = 32 --> take every 32nd index)\n",
    "    sampling_rate_ratio = orig_sampling_rate / new_sampling_rate\n",
    "\n",
    "    # location counter for the sequence tensor\n",
    "    loc_count = 0\n",
    "\n",
    "    # variables for progress printing\n",
    "    if print_progress:\n",
    "        prog_count = 0\n",
    "        max_count = len(data_points_df.csv_file.unique()) # number of unique csv-files\n",
    "        prev_progress = 0 # previous progress\n",
    "\n",
    "    # going through all csv-files (unique --> only once for each file)\n",
    "    for current_csv_file in data_points_df.csv_file.unique():\n",
    "\n",
    "        # join file path\n",
    "        file_path = os.path.join(csv_data_dir, current_csv_file)\n",
    "\n",
    "        # load the signal data of the current file\n",
    "        selected_data_df = pd.read_csv(file_path, skiprows=csv_skiprows, sep=csv_separator)\n",
    "        \n",
    "        # write data with selected signals to dict\n",
    "        selected_data = {}\n",
    "        for sig in signal_abbrs:\n",
    "            selected_data[sig] = selected_data_df.filter(regex=sig+'*').values\n",
    "            \n",
    "        # filter data with butterworth filter and save to new dictionary\n",
    "        selected_data_filt = {}\n",
    "        for sig in signal_abbrs:\n",
    "            selected_data_filt[sig] = fmpm.butter_lowpass_filter(selected_data[sig], \n",
    "                                                                 cutoff=cutoff, \n",
    "                                                                 fs=orig_sampling_rate, \n",
    "                                                                 order=order)\n",
    "    \n",
    "        # data frame with all repetitions of the current file\n",
    "        current_data_points = data_points_df.loc[data_points_df[csv_file_key] == current_csv_file]\n",
    "\n",
    "        # going through all repetitions of the current file\n",
    "        for ii in range(len(current_data_points)):\n",
    "            \n",
    "            # get start and stop indices of current data point (current repetition)\n",
    "            start_time = float(current_data_points.reset_index().loc[ii, start_time_key])\n",
    "            stop_time = float(current_data_points.reset_index().loc[ii, stop_time_key])\n",
    "            start_idx = round(start_time * orig_sampling_rate)\n",
    "            stop_idx = round(stop_time * orig_sampling_rate)\n",
    "\n",
    "            # consider the new sampling rate for signal data selection\n",
    "            new_indices = np.arange(start_idx, stop_idx, sampling_rate_ratio).round().astype(int)\n",
    "            \n",
    "            # check if array of new indices is longer than max_sequ_index\n",
    "            if len(new_indices) > max_sequ_index:\n",
    "                new_indices = new_indices[:max_sequ_index] # take only that much indices\n",
    "            \n",
    "            # add current sequences with new sampling rate\n",
    "            for kk, sig in enumerate(signal_abbrs):\n",
    "                for ll in range(len(signal_orientations)):\n",
    "                    # explanation: X_all[index_of_current_data_point, \n",
    "                    #                    select_all_until_length_of_new_signal_data, \n",
    "                    #                    index_of_current_signal (0...5)]\n",
    "                    X_all[loc_count,:len(new_indices),kk*len(signal_orientations)+ll] = \\\n",
    "                        selected_data_filt[sig][new_indices,ll]\n",
    "\n",
    "            # add current label\n",
    "            current_ex_abbr = current_data_points.reset_index().loc[ii,abbreviation_key]\n",
    "            y_all[loc_count] = labels_abbr2num_dict[current_ex_abbr]\n",
    "            \n",
    "            # add current sequence length\n",
    "            seqlens_all[loc_count] = len(new_indices)\n",
    "\n",
    "            loc_count += 1\n",
    "\n",
    "        # print progress of feauture generation\n",
    "        if print_progress:\n",
    "            prog_count += 1\n",
    "            prev_progress = print_progress_func(prog_count, max_count, prev_progress, add_info=progress_info)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    return X_all, y_all, seqlens_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:13:38.392765Z",
     "start_time": "2019-01-07T10:13:37.987741Z"
    }
   },
   "outputs": [],
   "source": [
    "class PhysioData_RNN():\n",
    "    '''\n",
    "    Class to handle signal data for RNN.\n",
    "    \n",
    "    Detailed description follows ...\n",
    "    \n",
    "    For now: look at docstrings of the following functions:\n",
    "        - load_data_from_database()\n",
    "        - select_data_points_from_df()\n",
    "        - generate_sequences_from_separate_repetitions()\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 test_subject_ids=-1,\n",
    "                 train_subject_ids=-1,\n",
    "                 test_rep_nums=-1,\n",
    "                 train_rep_nums=-1,\n",
    "                 test_ex_abbrs=-1,\n",
    "                 train_ex_abbrs=-1,\n",
    "                 with_non_Ex=True,\n",
    "                 max_sequence_length=6,\n",
    "                 orig_sampling_rate=256,\n",
    "                 new_sampling_rate=8,\n",
    "                 cutoff=10,\n",
    "                 order=6,\n",
    "                 csv_data_dir='E:\\Physio_Data_Split_Ex_and_NonEx',\n",
    "                 csv_skiprows=0,\n",
    "                 csv_separator=',',\n",
    "                 data_base_path='E:\\Jupyter_Notebooks\\Master_Project_Meinhart\\DataBase_Physio_with_nonEx.db',\n",
    "                 feat_save=False,\n",
    "                 feat_load_if_present=False,\n",
    "                 feat_save_dir='E:\\Physio_Features',\n",
    "                 print_progress=True,\n",
    "                 signal_abbrs=['Acc','Gyr'],\n",
    "                 signal_orientations=['x','y','z'],\n",
    "                 labels_abbr2num_dict={'RF':0,'RO':1,'RS':2,'LR':3,'BC':4,'TC':5,'MP':6,'SA':7,'P1':8,'P2':9,'NE':10},\n",
    "                 sub_id_key='subject_id',\n",
    "                 num_rep_key='num_rep',\n",
    "                 abbreviation_key='abbreviation',\n",
    "                 start_time_key='start_time',\n",
    "                 stop_time_key='stop_time',\n",
    "                 csv_file_key='csv_file'):\n",
    "        \n",
    "        # load all data from data points\n",
    "        self.all_data_points_df = load_data_from_database(data_base_path)\n",
    "        \n",
    "        # load data points for testing\n",
    "        self.test_data_points_df =  select_data_points_from_df(self.all_data_points_df,\n",
    "                                                               subject_ids=test_subject_ids,\n",
    "                                                               subject_ids_complementary=[],\n",
    "                                                               reps=test_rep_nums,\n",
    "                                                               abbrs=test_ex_abbrs,\n",
    "                                                               with_non_Ex=with_non_Ex,\n",
    "                                                               sub_id_key=sub_id_key,\n",
    "                                                               num_rep_key=num_rep_key,\n",
    "                                                               abbreviation_key=abbreviation_key)\n",
    "        \n",
    "        # load data points for training\n",
    "        self.train_data_points_df = select_data_points_from_df(self.all_data_points_df,\n",
    "                                                               subject_ids=train_subject_ids,\n",
    "                                                               subject_ids_complementary=test_subject_ids,\n",
    "                                                               reps=train_rep_nums,\n",
    "                                                               abbrs=train_ex_abbrs,\n",
    "                                                               with_non_Ex=with_non_Ex,\n",
    "                                                               sub_id_key=sub_id_key,\n",
    "                                                               num_rep_key=num_rep_key,\n",
    "                                                               abbreviation_key=abbreviation_key)\n",
    "        \n",
    "        # generate sequences for testing\n",
    "        self.X_test, self.y_test, self.seqlens_test = generate_sequences_from_separate_repetitions(\n",
    "                                                               self.test_data_points_df,\n",
    "                                                               max_sequence_length=max_sequence_length,\n",
    "                                                               orig_sampling_rate=orig_sampling_rate,\n",
    "                                                               new_sampling_rate=new_sampling_rate,\n",
    "                                                               cutoff=cutoff,\n",
    "                                                               order=order,\n",
    "                                                               csv_data_dir=csv_data_dir,\n",
    "                                                               csv_skiprows=csv_skiprows,\n",
    "                                                               csv_separator=csv_separator,\n",
    "                                                               signal_abbrs=signal_abbrs,\n",
    "                                                               signal_orientations=signal_orientations,\n",
    "                                                               labels_abbr2num_dict=labels_abbr2num_dict,\n",
    "                                                               abbreviation_key=abbreviation_key,\n",
    "                                                               start_time_key=start_time_key,\n",
    "                                                               stop_time_key=stop_time_key,\n",
    "                                                               csv_file_key=csv_file_key,\n",
    "                                                               print_progress=print_progress,\n",
    "                                                               progress_info='Generate sequences for testing...')\n",
    "        \n",
    "        # generate sequences for testing\n",
    "        self.X_train, self.y_train, self.seqlens_train = generate_sequences_from_separate_repetitions(\n",
    "                                                               self.train_data_points_df,\n",
    "                                                               max_sequence_length=max_sequence_length,\n",
    "                                                               orig_sampling_rate=orig_sampling_rate,\n",
    "                                                               new_sampling_rate=new_sampling_rate,\n",
    "                                                               cutoff=cutoff,\n",
    "                                                               order=order,\n",
    "                                                               csv_data_dir=csv_data_dir,\n",
    "                                                               csv_skiprows=csv_skiprows,\n",
    "                                                               csv_separator=csv_separator,\n",
    "                                                               signal_abbrs=signal_abbrs,\n",
    "                                                               signal_orientations=signal_orientations,\n",
    "                                                               labels_abbr2num_dict=labels_abbr2num_dict,\n",
    "                                                               abbreviation_key=abbreviation_key,\n",
    "                                                               start_time_key=start_time_key,\n",
    "                                                               stop_time_key=stop_time_key,\n",
    "                                                               csv_file_key=csv_file_key,\n",
    "                                                               print_progress=print_progress,\n",
    "                                                               progress_info='Generate sequences for training...')\n",
    "    \n",
    "    \n",
    "    def get_train_batch(self, batch_size):\n",
    "        '''\n",
    "        Method to get batch with randomly selected training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Number of data points in batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train_batch, y_train_batch, seqlens_train_batch\n",
    "        '''\n",
    "        \n",
    "        instance_indices = list(range(len(self.y_train))) # list with all train indices\n",
    "        np.random.shuffle(instance_indices) # shuffle the train indices\n",
    "        batch_indices = instance_indices[:batch_size] # randomly select train indices\n",
    "    \n",
    "        # select batch data with corresponding indices\n",
    "        X_train_batch = np.array(self.X_train)[batch_indices]\n",
    "        y_train_batch = self.y_train[batch_indices]\n",
    "        seqlens_train_batch = np.array(self.seqlens_train)[batch_indices]\n",
    "\n",
    "        return X_train_batch, y_train_batch, seqlens_train_batch\n",
    "\n",
    "    \n",
    "    # methods to get data\n",
    "    def get_X_test(self):\n",
    "        return self.X_test\n",
    "    \n",
    "    def get_y_test(self):\n",
    "        return self.y_test\n",
    "    \n",
    "    def get_seqlens_test(self):\n",
    "        return self.seqlens_test\n",
    "    \n",
    "    def get_X_train(self):\n",
    "        return self.X_train\n",
    "    \n",
    "    def get_y_train(self):\n",
    "        return self.y_train\n",
    "    \n",
    "    def get_sequlens_train(self):\n",
    "        return self.sequlens_train\n",
    "    \n",
    "    \n",
    "    # methods to get data points (DataFrames)\n",
    "    def get_test_data_points(self):\n",
    "        return self.test_data_points_df\n",
    "    \n",
    "    def get_train_data_points(self):\n",
    "        return self.train_data_points_df\n",
    "    \n",
    "    def get_all_data_points(self):\n",
    "        return self.all_data_points_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instance of the class *PhysioData_RNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.232329Z",
     "start_time": "2019-01-07T10:13:38.397765Z"
    }
   },
   "outputs": [],
   "source": [
    "PD_RNN1 = PhysioData_RNN(test_subject_ids=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.261331Z",
     "start_time": "2019-01-07T10:14:23.238330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 48, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(PD_RNN1.get_X_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.475343Z",
     "start_time": "2019-01-07T10:14:23.266331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6120, 48, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(PD_RNN1.get_X_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.657354Z",
     "start_time": "2019-01-07T10:14:23.488344Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.85008976e-01,  3.73013764e-01,  2.24020535e-01,\n",
       "         1.21751042e+01,  1.52829258e+00,  9.10124605e+00],\n",
       "       [-9.04367908e-01,  8.26654823e-01,  3.67851136e-01,\n",
       "        -1.01565373e+01,  5.38131320e+00,  7.39012265e+01],\n",
       "       [-1.01528357e+00,  1.07474501e+00,  3.88348309e-01,\n",
       "        -2.42753698e+01, -2.82604112e+01,  1.39737623e+02],\n",
       "       [-1.03430703e+00,  1.15137975e+00,  2.56821977e-01,\n",
       "         1.23076905e+01, -3.72862596e+01,  1.91338412e+02],\n",
       "       [-6.34748614e-01,  9.63927847e-01,  2.02384178e-01,\n",
       "        -7.54283377e+00, -3.91051296e+01,  1.58483358e+02],\n",
       "       [-2.93779369e-01,  7.57520082e-01,  2.75981183e-01,\n",
       "        -5.35511185e+00, -2.56590433e+01,  1.41178164e+02],\n",
       "       [ 1.04911864e-01,  5.41331501e-01,  2.45186189e-01,\n",
       "         4.44534291e+00, -1.63078708e+01,  1.13614763e+02],\n",
       "       [ 4.32216915e-01,  4.15494757e-01,  2.65919449e-01,\n",
       "         1.68503415e+00, -2.25416173e+00,  6.80021935e+01],\n",
       "       [ 6.11687397e-01,  4.31368824e-01,  2.80606846e-01,\n",
       "        -1.46211643e+01, -3.62546694e+00,  1.43849657e+01],\n",
       "       [ 6.21065955e-01,  5.10839768e-01,  3.34571072e-01,\n",
       "         2.23573633e+00,  1.75674248e+00, -4.89205622e-02],\n",
       "       [ 5.76487955e-01,  4.50268683e-01,  3.27837728e-01,\n",
       "        -1.50772590e+00,  2.06181750e+00, -1.68996313e+01],\n",
       "       [ 4.09856906e-01,  3.94621483e-01,  2.27177439e-01,\n",
       "         3.95772234e+00,  2.69890721e+00, -6.31377488e+01],\n",
       "       [ 1.60316970e-01,  5.17148631e-01,  2.94741576e-01,\n",
       "        -2.82360611e+01,  2.85425109e+01, -1.19222601e+02],\n",
       "       [-1.91762490e-01,  7.73961019e-01,  3.24831739e-01,\n",
       "         2.83235200e+01,  2.91210772e+01, -1.39990128e+02],\n",
       "       [-5.50248195e-01,  1.01577925e+00,  3.24931914e-01,\n",
       "         8.48018451e+00,  1.93014588e+01, -1.57620844e+02],\n",
       "       [-9.05278854e-01,  9.07200894e-01,  3.87678549e-01,\n",
       "        -2.45637713e+01,  2.33728910e+01, -1.54579808e+02],\n",
       "       [-1.06081757e+00,  8.50847948e-01,  3.23734638e-01,\n",
       "        -7.80314506e+00,  2.77927524e+01, -1.22591467e+02],\n",
       "       [-9.83693089e-01,  7.31427570e-01,  3.45395706e-01,\n",
       "         3.35026007e+01,  1.54412798e+01, -1.12633149e+02],\n",
       "       [-1.03250496e+00,  4.24830007e-01,  3.33556145e-01,\n",
       "        -1.44286634e+01,  2.42460802e+01, -4.55003833e+01],\n",
       "       [-8.73047459e-01,  3.91488597e-01,  2.36033804e-01,\n",
       "         2.69888523e+01,  8.09433261e+00, -1.69945675e+01],\n",
       "       [-9.44795597e-01,  2.54712710e-01,  2.17821080e-01,\n",
       "         5.63878809e+00,  2.21768665e+00, -3.30126554e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD_RNN1.get_X_test()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.826363Z",
     "start_time": "2019-01-07T10:14:23.666354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD_RNN1.get_y_test()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:23.996373Z",
     "start_time": "2019-01-07T10:14:23.838364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD_RNN1.get_seqlens_test()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:24.150382Z",
     "start_time": "2019-01-07T10:14:24.002373Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_batch, y_train_batch, seqlens_train_batch = PD_RNN1.get_train_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:24.245387Z",
     "start_time": "2019-01-07T10:14:24.153382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 48, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:24.382395Z",
     "start_time": "2019-01-07T10:14:24.265388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8, 10, 10,  7], dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:24.523403Z",
     "start_time": "2019-01-07T10:14:24.391396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 17, 16, 28, 24])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Recurrent Neural Network with *TensorFlow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:14:34.424969Z",
     "start_time": "2019-01-07T10:14:24.536404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programme\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:52:53.820845Z",
     "start_time": "2019-01-07T17:52:53.803844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=E:\\Jupyter_Notebooks\\Master_Project_Meinhart_git\\logs\\RNN_with_summaries\n"
     ]
    }
   ],
   "source": [
    "# directory to save TensorBoard model summaries\n",
    "LOG_DIR_ALL = 'E:\\Jupyter_Notebooks\\Master_Project_Meinhart_git\\logs\\RNN_with_summaries'\n",
    "\n",
    "# run the command:\n",
    "print('tensorboard --logdir=' + LOG_DIR_ALL)\n",
    "\n",
    "#  --> open http://FlorianMeinhart:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:52:53.975854Z",
     "start_time": "2019-01-07T17:52:53.831846Z"
    }
   },
   "outputs": [],
   "source": [
    "# define parameters for RNN\n",
    "num_signals = 6 # number of signals (Acc: x, y, z; Gyr: x, y, z)\n",
    "num_classes = 11 # number of classes\n",
    "batch_size = 500 # number of data points for each batch\n",
    "time_steps = None # --> dynamic_rnn\n",
    "hidden_layer_size_rnn = 128 # number of neurons in the hidden layer of the RNN\n",
    "num_steps = 301 # number of steps for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:52:54.143863Z",
     "start_time": "2019-01-07T17:52:53.983854Z"
    }
   },
   "outputs": [],
   "source": [
    "# get time in order to append corresponding string to log directory\n",
    "now = datetime.now()\n",
    "LOG_DIR_TRAIN = LOG_DIR_ALL + now.strftime('\\%Y%m%d-%H%M%S' + '_train')\n",
    "LOG_DIR_TEST = LOG_DIR_ALL + now.strftime('\\%Y%m%d-%H%M%S' + '_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:52:54.912907Z",
     "start_time": "2019-01-07T17:52:54.152864Z"
    }
   },
   "outputs": [],
   "source": [
    "# create new tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, time_steps, num_signals], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, num_classes], name='labels')\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[None], name='seqlens')\n",
    "\n",
    "with tf.name_scope('RNN_layer'):\n",
    "    #rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_layer_size)\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size_rnn)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, inputs, sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('linear_layer'):\n",
    "    W1 = tf.Variable(tf.truncated_normal([hidden_layer_size_rnn, num_classes], mean=0, stddev=0.1), name='weights_linear')\n",
    "    b1 = tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=0.1), name='biases_linear')\n",
    "    #final_output = tf.matmul(states, W1) + b1\n",
    "    final_output = tf.matmul(states[0], W1) + b1\n",
    "    \n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_output, labels=labels)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    correct_prediction = tf.equal(tf.argmax(labels,1), tf.argmax(final_output,1), name='correct_prediction')\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# merge summaries and create summary writers for training and testing\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR_TRAIN)\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:52:54.939909Z",
     "start_time": "2019-01-07T17:52:54.921908Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    '''\n",
    "    Convert lables to one-hot format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array or list of integers\n",
    "        --> e.g. [0, 1, 4, 2, 0]\n",
    "        \n",
    "    num_classes : int\n",
    "        Number of classes.\n",
    "        (Has to be >= max(labels))\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix\n",
    "        Lables in one-hot format.\n",
    "    \n",
    "        --> e.g.\n",
    "              [[1., 0., 0., 0., 0.],\n",
    "               [0., 1., 0., 0., 0.],\n",
    "               [0., 0., 0., 0., 1.],\n",
    "               [0., 0., 1., 0., 0.],\n",
    "               [1., 0., 0., 0., 0.]]\n",
    "    '''\n",
    "    n = len(labels)\n",
    "    labels_one_hot = np.zeros((n, num_classes))\n",
    "    labels_one_hot[range(n), labels] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:55:56.901317Z",
     "start_time": "2019-01-07T17:52:54.944909Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 0\n",
      "\tTrain Set: 13.400\n",
      "\tTest Set:  9.591\n",
      "Accuracy at step 20\n",
      "\tTrain Set: 60.800\n",
      "\tTest Set:  53.738\n",
      "Accuracy at step 40\n",
      "\tTrain Set: 74.600\n",
      "\tTest Set:  77.715\n",
      "Accuracy at step 60\n",
      "\tTrain Set: 84.600\n",
      "\tTest Set:  81.382\n",
      "Accuracy at step 80\n",
      "\tTrain Set: 87.000\n",
      "\tTest Set:  85.049\n",
      "Accuracy at step 100\n",
      "\tTrain Set: 90.200\n",
      "\tTest Set:  88.858\n",
      "Accuracy at step 120\n",
      "\tTrain Set: 91.200\n",
      "\tTest Set:  90.691\n",
      "Accuracy at step 140\n",
      "\tTrain Set: 96.200\n",
      "\tTest Set:  92.525\n",
      "Accuracy at step 160\n",
      "\tTrain Set: 95.600\n",
      "\tTest Set:  95.063\n",
      "Accuracy at step 180\n",
      "\tTrain Set: 98.600\n",
      "\tTest Set:  93.794\n",
      "Accuracy at step 200\n",
      "\tTrain Set: 97.800\n",
      "\tTest Set:  94.358\n",
      "Accuracy at step 220\n",
      "\tTrain Set: 97.800\n",
      "\tTest Set:  93.230\n",
      "Accuracy at step 240\n",
      "\tTrain Set: 98.800\n",
      "\tTest Set:  95.487\n",
      "Accuracy at step 260\n",
      "\tTrain Set: 98.800\n",
      "\tTest Set:  95.346\n",
      "Accuracy at step 280\n",
      "\tTrain Set: 98.200\n",
      "\tTest Set:  95.487\n",
      "Accuracy at step 300\n",
      "\tTrain Set: 97.600\n",
      "\tTest Set:  96.192\n"
     ]
    }
   ],
   "source": [
    "# open a session for the created graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    test_writer.add_graph(sess.graph)\n",
    "    \n",
    "    # get test labels in one-hot format\n",
    "    y_test_all_one_hot = one_hot(PD_RNN1.get_y_test(), num_classes)\n",
    "    \n",
    "    # get sequence tensor for testing and corresponding sequence lengths\n",
    "    X_test_all = PD_RNN1.get_X_test()\n",
    "    seqlens_test_all = PD_RNN1.get_seqlens_test()\n",
    "    \n",
    "    # train the RNN with a defined number of steps\n",
    "    for step in range(num_steps):\n",
    "        # get batch data\n",
    "        x_batch, y_batch, seqlens_batch = PD_RNN1.get_train_batch(batch_size)\n",
    "        \n",
    "        # get batch labels in one-hot format\n",
    "        y_batch_one_hot = one_hot(y_batch, num_classes)\n",
    "        \n",
    "        # run the session\n",
    "        sess.run(train_step, feed_dict={inputs:x_batch, labels:y_batch_one_hot, seqlens:seqlens_batch})\n",
    "        \n",
    "        # evaluate the RNN every 20 steps\n",
    "        if step % 20 == 0:\n",
    "            summary_train, accuracy_train = sess.run([merged, accuracy], \n",
    "                                                      feed_dict={inputs:x_batch, \n",
    "                                                                 labels:y_batch_one_hot, \n",
    "                                                                 seqlens:seqlens_batch})\n",
    "            print('Accuracy at step {}'.format(step))\n",
    "            print('\\tTrain Set: {:.3f}'.format(accuracy_train))\n",
    "            \n",
    "            # write to train summary\n",
    "            train_writer.add_summary(summary_train, step)\n",
    "    \n",
    "            summary_test, batch_pred, accuracy_test = sess.run([merged, tf.argmax(final_output,1), accuracy],\n",
    "                                                                feed_dict={inputs:X_test_all, \n",
    "                                                                           labels:y_test_all_one_hot, \n",
    "                                                                           seqlens:seqlens_test_all})\n",
    "            # write to test summary\n",
    "            test_writer.add_summary(summary_test, step)\n",
    "    \n",
    "            print('\\tTest Set:  {:.3f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:55:57.053325Z",
     "start_time": "2019-01-07T17:55:56.905317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise\tPrecision [%]\tRecall [%]\tAccuracy[%]\n",
      "  RF\t\t  100.00\t   50.00\t   97.88\n",
      "  RO\t\t   65.91\t   96.67\t   97.74\n",
      "  RS\t\t   84.85\t   93.33\t   99.01\n",
      "  LR\t\t  100.00\t  100.00\t  100.00\n",
      "  BC\t\t  100.00\t  100.00\t  100.00\n",
      "  TC\t\t  100.00\t  100.00\t  100.00\n",
      "  MP\t\t   96.67\t   96.67\t   99.72\n",
      "  SA\t\t   96.88\t  100.00\t   99.86\n",
      "  P1\t\t  100.00\t  100.00\t  100.00\n",
      "  P2\t\t   89.29\t   83.33\t   98.87\n",
      "  NE\t\t   99.51\t   99.26\t   99.29\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_accuracy(batch_pred, PD_RNN1.get_y_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:55:57.101328Z",
     "start_time": "2019-01-07T17:55:57.057325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 misclassified (709 test data points):\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RF classified as RO\n",
      "RO classified as NE\n",
      "RS classified as P2\n",
      "RS classified as P2\n",
      "MP classified as NE\n",
      "P2 classified as RS\n",
      "P2 classified as RS\n",
      "P2 classified as RS\n",
      "P2 classified as RS\n",
      "P2 classified as RS\n",
      "NE classified as P2\n",
      "NE classified as SA\n",
      "NE classified as MP\n"
     ]
    }
   ],
   "source": [
    "print_misclassified_data_points(batch_pred, PD_RNN1.get_y_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
