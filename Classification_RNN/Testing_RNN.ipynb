{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:14.851137Z",
     "start_time": "2019-01-03T00:19:12.784019Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') # in order to import modules from my own package\n",
    "from packageMeinhart import functionsMasterProjectMeinhart as fmpm\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_precision_recall_accuracy\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_misclassified_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:17.250274Z",
     "start_time": "2019-01-03T00:19:15.577179Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programme\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:23.514633Z",
     "start_time": "2019-01-03T00:19:23.507632Z"
    }
   },
   "outputs": [],
   "source": [
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:24.398683Z",
     "start_time": "2019-01-03T00:19:24.371682Z"
    }
   },
   "outputs": [],
   "source": [
    "# define some parameters\n",
    "element_size = 6\n",
    "time_steps = 48 # number of steps for 6 s at 8 Hz\n",
    "num_classes = 11\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:24.815707Z",
     "start_time": "2019-01-03T00:19:24.807707Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(vec, vals=num_classes):\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:25.200729Z",
     "start_time": "2019-01-03T00:19:25.194729Z"
    }
   },
   "outputs": [],
   "source": [
    "#one_hot(y_test_all.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:26.822822Z",
     "start_time": "2019-01-03T00:19:26.448800Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all data, except data from one subject (test data)\n",
    "test_data_subject = 1\n",
    "\n",
    "db_name='E:\\Jupyter_Notebooks\\Master_Project_Meinhart\\DataBase_Physio_with_nonEx.db' # database name\n",
    "exercise_abbrs = ['RF','RO','RS','LR','BC','TC','MP','SA','P1','P2','NE'] # exercise abbreviations\n",
    "# Connect to an existing database\n",
    "conn = sqlite3.connect(db_name)\n",
    "cur = conn.cursor()\n",
    "train_data_points = {} # dictionary with the exercise abbreviation as key\n",
    "test_data_points = {}\n",
    "\n",
    "for key in exercise_abbrs:\n",
    "    # sql command to extract data\n",
    "    query_sql = \"\"\"\n",
    "        SELECT r.start_time, r.stop_time, e.csv_file\n",
    "        FROM subjects s\n",
    "        INNER JOIN exercises e\n",
    "        ON s.id = e.subject_id\n",
    "        INNER JOIN paradigms p\n",
    "        ON p.id = e.paradigm_id\n",
    "        INNER JOIN repetitions r\n",
    "        ON e.id = r.exercise_id\n",
    "        WHERE p.abbreviation = '{}'\n",
    "        AND NOT s.id = {}\n",
    "        \"\"\".format(key, test_data_subject)\n",
    "    # get data from data base and close connection\n",
    "    train_data_points[key] = pd.read_sql_query(query_sql, conn)\n",
    "\n",
    "for key in exercise_abbrs:\n",
    "    # sql command to extract data\n",
    "    query_sql = \"\"\"\n",
    "        SELECT r.start_time, r.stop_time, e.csv_file\n",
    "        FROM subjects s\n",
    "        INNER JOIN exercises e\n",
    "        ON s.id = e.subject_id\n",
    "        INNER JOIN paradigms p\n",
    "        ON p.id = e.paradigm_id\n",
    "        INNER JOIN repetitions r\n",
    "        ON e.id = r.exercise_id\n",
    "        WHERE p.abbreviation = '{}'\n",
    "        AND s.id = {}\n",
    "        \"\"\".format(key, test_data_subject)\n",
    "    # get data from data base and close connection\n",
    "    test_data_points[key] = pd.read_sql_query(query_sql, conn)\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:28.699929Z",
     "start_time": "2019-01-03T00:19:28.666927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points (repetitions) for training:\n",
      "RF:\t239\n",
      "RO:\t240\n",
      "RS:\t240\n",
      "LR:\t241\n",
      "BC:\t242\n",
      "TC:\t243\n",
      "MP:\t242\n",
      "SA:\t242\n",
      "P1:\t240\n",
      "P2:\t239\n",
      "NE:\t3712\n",
      "total:\t6120\n",
      "\n",
      "Number of data points (repetitions) for testing:\n",
      "RF:\t30\n",
      "RO:\t30\n",
      "RS:\t30\n",
      "LR:\t30\n",
      "BC:\t31\n",
      "TC:\t30\n",
      "MP:\t30\n",
      "SA:\t31\n",
      "P1:\t30\n",
      "P2:\t30\n",
      "NE:\t407\n",
      "total:\t709\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points (repetitions) for training:')\n",
    "count = 0\n",
    "for key in exercise_abbrs:\n",
    "    print(key + ':\\t' + str(train_data_points[key].shape[0]))\n",
    "    count += train_data_points[key].shape[0]\n",
    "print('total:\\t' + str(count))\n",
    "\n",
    "print('\\nNumber of data points (repetitions) for testing:')\n",
    "count = 0\n",
    "for key in exercise_abbrs:\n",
    "    print(key + ':\\t' + str(test_data_points[key].shape[0]))\n",
    "    count += test_data_points[key].shape[0]\n",
    "print('total:\\t' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:40.620611Z",
     "start_time": "2019-01-03T00:19:40.594610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>csv_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.6097522701321</td>\n",
       "      <td>subject02_00_nonEx.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.6097522701321</td>\n",
       "      <td>5.98056861437206</td>\n",
       "      <td>subject02_00_nonEx.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.98056861437206</td>\n",
       "      <td>7.84471642992804</td>\n",
       "      <td>subject02_00_nonEx.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.84471642992804</td>\n",
       "      <td>12.3377339822144</td>\n",
       "      <td>subject02_00_nonEx.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.3377339822144</td>\n",
       "      <td>15.5979262935134</td>\n",
       "      <td>subject02_00_nonEx.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_time         stop_time                csv_file\n",
       "0                 0   3.6097522701321  subject02_00_nonEx.csv\n",
       "1   3.6097522701321  5.98056861437206  subject02_00_nonEx.csv\n",
       "2  5.98056861437206  7.84471642992804  subject02_00_nonEx.csv\n",
       "3  7.84471642992804  12.3377339822144  subject02_00_nonEx.csv\n",
       "4  12.3377339822144  15.5979262935134  subject02_00_nonEx.csv"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head of one loaded data frame as an example:\n",
    "train_data_points['NE'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:19:53.037321Z",
     "start_time": "2019-01-03T00:19:53.017320Z"
    }
   },
   "outputs": [],
   "source": [
    "# dictionary for labels\n",
    "ex_abbr2ind = { 'RF':0,\n",
    "                'RO':1,\n",
    "                'RS':2,\n",
    "                'LR':3,\n",
    "                'BC':4,\n",
    "                'TC':5,\n",
    "                'MP':6,\n",
    "                'SA':7,\n",
    "                'P1':8,\n",
    "                'P2':9,\n",
    "                'NE':10}\n",
    "\n",
    "ex_ind2abbr = {index: abbr for abbr, index in ex_abbr2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:20:09.266249Z",
     "start_time": "2019-01-03T00:20:09.243248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex_abbr2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:20:10.785336Z",
     "start_time": "2019-01-03T00:20:10.761335Z"
    }
   },
   "outputs": [],
   "source": [
    "# putting all train data and labels together\n",
    "all_train_data = np.concatenate([train_data_points[ex_ind2abbr[ii]] for ii in range(len(ex_ind2abbr))], axis=0)\n",
    "\n",
    "y_train_all = np.zeros(np.shape(all_train_data)[0])\n",
    "start_ind = 0\n",
    "for ii in range(len(ex_ind2abbr)):\n",
    "    stop_ind = len(train_data_points[ex_ind2abbr[ii]]) + start_ind\n",
    "    y_train_all[start_ind:stop_ind] = ii\n",
    "    start_ind = stop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:20:11.567381Z",
     "start_time": "2019-01-03T00:20:11.553380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T00:20:12.333425Z",
     "start_time": "2019-01-03T00:20:12.323424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.6509139384920637' '3.911928323412699' 'subject02_RF_05.csv']\n",
      " ['3.911928323412699' '7.031159474206351' 'subject02_RF_05.csv']\n",
      " ['7.031159474206351' '10.398511284722224' 'subject02_RF_05.csv']]\n"
     ]
    }
   ],
   "source": [
    "print(all_train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T22:54:32.229233Z",
     "start_time": "2018-12-26T22:54:32.073224Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_dir  = 'E:\\Physio_Features'\n",
    "X_train_name = 'RNN_X_train_without_subject{0:02}_8Hz.csv'.format(\n",
    "                    test_data_subject)\n",
    "y_train_name = 'RNN_y_train_without_subject{0:02}_8Hz.csv'.format(\n",
    "                    test_data_subject)\n",
    "seqlens_train_name = 'RNN_seqlens_train_without_subject{0:02}_8Hz.csv'.format(\n",
    "                    test_data_subject)\n",
    "\n",
    "X_train_path = os.path.join(X_train_dir, X_train_name)\n",
    "y_train_path = os.path.join(X_train_dir, y_train_name)\n",
    "seqlens_train_path = os.path.join(X_train_dir, seqlens_train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T22:54:34.475361Z",
     "start_time": "2018-12-26T22:54:32.234233Z"
    }
   },
   "outputs": [],
   "source": [
    "# only generate the train data if they do not already exist\n",
    "if not os.path.isfile(X_train_path):\n",
    "    \n",
    "    # Generating one corresponding matrix for train data\n",
    "    X_train_all = []\n",
    "    seqlens_train_all = []\n",
    "\n",
    "    factor_256_to_8Hz = 32\n",
    "    steps_6s_256Hz = 6 * 256\n",
    "\n",
    "    # directory of csv file\n",
    "    csv_dir='E:\\Physio_Data_Split_Ex_and_NonEx'\n",
    "\n",
    "    for ii in range(len(all_train_data)):\n",
    "\n",
    "        data = all_train_data[ii]\n",
    "\n",
    "        signal_data = fmpm.get_sensor_data(os.path.join(csv_dir, data[2]),\n",
    "                    signals=['Acc','Gyr'], \n",
    "                    sampling_rate=256,\n",
    "                    start_time=float(data[0]), \n",
    "                    stop_time=float(data[1]))\n",
    "\n",
    "        X_one = np.zeros((time_steps, element_size))\n",
    "        seqlens_train_all.append(int(len(signal_data['Acc'][:,0]) / factor_256_to_8Hz)) # all columns have same length\n",
    "\n",
    "        col_inc = 0\n",
    "        for sig in ['Acc','Gyr']:\n",
    "            for col in [0,1,2]:\n",
    "                puffer_6s_256Hz = np.zeros(steps_6s_256Hz) # 6 s at sampling rate 256 Hz\n",
    "                puffer_6s_256Hz[:len(signal_data[sig][:,col])] = signal_data[sig][:,col]\n",
    "                puffer_6s_8Hz = puffer_6s_256Hz.reshape(-1, factor_256_to_8Hz).mean(axis=1)\n",
    "                X_one[:,col+col_inc] = puffer_6s_8Hz\n",
    "\n",
    "            col_inc += 3\n",
    "\n",
    "        X_train_all.append(X_one)\n",
    "    \n",
    "    X_train_all_save = np.array(X_train_all).reshape(6120,-1)\n",
    "    \n",
    "    np.savetxt(X_train_path, X_train_all_save, delimiter=\";\")\n",
    "    np.savetxt(y_train_path, y_train_all, delimiter=\";\")\n",
    "    np.savetxt(seqlens_train_path, seqlens_train_all, delimiter=\";\")\n",
    "    \n",
    "# otherwise load them\n",
    "else:\n",
    "    X_train_all_loaded = np.loadtxt(open(X_train_path), delimiter=\";\")\n",
    "    X_train_all = X_train_all_loaded.reshape(-1, time_steps, element_size)\n",
    "\n",
    "    y_train_all = np.loadtxt(open(y_train_path), delimiter=\";\")\n",
    "    seqlens_train_all = np.loadtxt(open(seqlens_train_path), delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:13:04.217743Z",
     "start_time": "2018-12-27T00:13:04.189742Z"
    }
   },
   "outputs": [],
   "source": [
    "# putting all test data and labels together\n",
    "all_test_data = np.concatenate([test_data_points[ex_ind2abbr[ii]] for ii in range(len(ex_ind2abbr))], axis=0)\n",
    "\n",
    "y_test_all = np.zeros(np.shape(all_test_data)[0])\n",
    "start_ind = 0\n",
    "for ii in range(len(ex_ind2abbr)):\n",
    "    stop_ind = len(test_data_points[ex_ind2abbr[ii]]) + start_ind\n",
    "    y_test_all[start_ind:stop_ind] = ii\n",
    "    start_ind = stop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:25:07.940138Z",
     "start_time": "2018-12-27T00:24:40.663578Z"
    }
   },
   "outputs": [],
   "source": [
    "# define some parameters\n",
    "element_size = 6\n",
    "time_steps = 48 # number of steps for 6 s at 8 Hz\n",
    "num_classes = 11\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Generating one corresponding matrix for test data\n",
    "X_test_all = []\n",
    "seqlens_test_all = []\n",
    "\n",
    "factor_256_to_8Hz = 32\n",
    "steps_6s_256Hz = 6 * 256\n",
    "\n",
    "# directory of csv file\n",
    "csv_dir='E:\\Physio_Data_Split_Ex_and_NonEx'\n",
    "\n",
    "for ii in range(len(all_test_data)):\n",
    "\n",
    "    data = all_test_data[ii]\n",
    "\n",
    "    signal_data = fmpm.get_sensor_data(os.path.join(csv_dir, data[2]),\n",
    "                signals=['Acc','Gyr'], \n",
    "                sampling_rate=256,\n",
    "                start_time=float(data[0]), \n",
    "                stop_time=float(data[1]))\n",
    "\n",
    "    X_one = np.zeros((time_steps, element_size))\n",
    "    seqlens_test_all.append(int(len(signal_data['Acc'][:,0]) / factor_256_to_8Hz)) # all columns have same length\n",
    "\n",
    "    col_inc = 0\n",
    "    for sig in ['Acc','Gyr']:\n",
    "        for col in [0,1,2]:\n",
    "            puffer_6s_256Hz = np.zeros(steps_6s_256Hz) # 6 s at sampling rate 256 Hz\n",
    "            puffer_6s_256Hz[:len(signal_data[sig][:,col])] = signal_data[sig][:,col]\n",
    "            puffer_6s_8Hz = puffer_6s_256Hz.reshape(-1, factor_256_to_8Hz).mean(axis=1)\n",
    "            X_one[:,col+col_inc] = puffer_6s_8Hz\n",
    "\n",
    "        col_inc += 3\n",
    "\n",
    "    X_test_all.append(X_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T22:55:01.443904Z",
     "start_time": "2018-12-26T22:55:01.432903Z"
    }
   },
   "outputs": [],
   "source": [
    "np.shape(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:28:56.505211Z",
     "start_time": "2018-12-27T00:28:29.173648Z"
    }
   },
   "outputs": [],
   "source": [
    "# define some parameters\n",
    "element_size = 6\n",
    "time_steps = 96 # number of steps for 6 s at 16 Hz\n",
    "num_classes = 11\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Generating one corresponding matrix for test data\n",
    "X_test_all = []\n",
    "seqlens_test_all = []\n",
    "\n",
    "factor_256_to_8Hz = 16\n",
    "steps_6s_256Hz = 6 * 256\n",
    "\n",
    "# directory of csv file\n",
    "csv_dir='E:\\Physio_Data_Split_Ex_and_NonEx'\n",
    "\n",
    "for ii in range(len(all_test_data)):\n",
    "\n",
    "    data = all_test_data[ii]\n",
    "\n",
    "    signal_data = fmpm.get_sensor_data(os.path.join(csv_dir, data[2]),\n",
    "                signals=['Acc','Gyr'], \n",
    "                sampling_rate=256,\n",
    "                start_time=float(data[0]), \n",
    "                stop_time=float(data[1]))\n",
    "\n",
    "    X_one = np.zeros((time_steps, element_size))\n",
    "    seqlens_test_all.append(int(len(signal_data['Acc'][:,0]) / factor_256_to_8Hz)) # all columns have same length\n",
    "\n",
    "    col_inc = 0\n",
    "    for sig in ['Acc','Gyr']:\n",
    "        for col in [0,1,2]:\n",
    "            puffer_6s_256Hz = np.zeros(steps_6s_256Hz) # 6 s at sampling rate 256 Hz\n",
    "            puffer_6s_256Hz[:len(signal_data[sig][:,col])] = signal_data[sig][:,col]\n",
    "            puffer_6s_8Hz = puffer_6s_256Hz.reshape(-1, factor_256_to_8Hz).mean(axis=1)\n",
    "            X_one[:,col+col_inc] = puffer_6s_8Hz\n",
    "\n",
    "        col_inc += 3\n",
    "\n",
    "    X_test_all.append(X_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:11.646077Z",
     "start_time": "2018-12-27T00:29:11.627076Z"
    }
   },
   "outputs": [],
   "source": [
    "np.shape(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T21:31:36.050041Z",
     "start_time": "2018-12-16T21:31:36.044040Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:19.910550Z",
     "start_time": "2018-12-27T00:29:19.886548Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size, X_train_all, y_train_all, seqlens_train_all):\n",
    "    \n",
    "    instance_indices = list(range(len(all_train_data)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch_indices = instance_indices[:batch_size]\n",
    "\n",
    "    X = np.array(X_train_all)[batch_indices]\n",
    "    y = y_train_all[batch_indices]\n",
    "    seqlens = np.array(seqlens_train_all)[batch_indices]\n",
    "    \n",
    "    return X, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:20.300572Z",
     "start_time": "2018-12-27T00:29:20.275571Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y, seqlens = get_train_batch(batch_size, X_train_all, y_train_all, seqlens_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:20.659593Z",
     "start_time": "2018-12-27T00:29:20.649592Z"
    }
   },
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:21.766656Z",
     "start_time": "2018-12-27T00:29:21.753655Z"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:29:26.154907Z",
     "start_time": "2018-12-27T00:29:26.139906Z"
    }
   },
   "outputs": [],
   "source": [
    "# where to save TensorBoard model summaries\n",
    "\n",
    "LOG_DIR_ALL = \"logs/RNN_with_summaries\"\n",
    "\n",
    "# tensorboard --logdir=logs/RNN_with_summaries\n",
    "\n",
    "#  http://FlorianMeinhart:6006\n",
    "\n",
    "# define some parameters\n",
    "#element_size = 6\n",
    "#time_steps = 48 # number of steps for 6 s at 8 Hz\n",
    "#num_classes = 11\n",
    "#batch_size = 128\n",
    "#hidden_layer_size = 128\n",
    "\n",
    "batch_size = 256\n",
    "time_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T22:43:36.468726Z",
     "start_time": "2018-12-26T22:43:36.456725Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:37:29.288541Z",
     "start_time": "2018-12-27T00:37:28.700507Z"
    }
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "LOG_DIR_TRAIN = LOG_DIR_ALL + now.strftime('/%Y%m%d-%H%M%S' + '_train')\n",
    "LOG_DIR_TEST = LOG_DIR_ALL + now.strftime('/%Y%m%d-%H%M%S' + '_test')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, time_steps, element_size], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, num_classes], name='labels')\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[None], name='seqlens')\n",
    "\n",
    "with tf.name_scope('RNN_layer'):\n",
    "    #rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_layer_size)\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, inputs, sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('linear_layer'):\n",
    "    W1 = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean=0, stddev=0.1), name='weights_linear')\n",
    "    b1 = tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=0.1), name='biases_linear')\n",
    "    #final_output = tf.matmul(states, W1) + b1\n",
    "    final_output = tf.matmul(states[0], W1) + b1\n",
    "    \n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_output, labels=labels)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    correct_prediction = tf.equal(tf.argmax(labels,1), tf.argmax(final_output,1), name='correct_prediction')\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR_TRAIN)\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR_TEST)\n",
    "\n",
    "print('tensorboard --logdir=' + LOG_DIR_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:42:46.476683Z",
     "start_time": "2018-12-27T00:37:29.339543Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    test_writer.add_graph(sess.graph)\n",
    "    \n",
    "    y_test_all_one_hot = one_hot(y_test_all.astype(int), vals=num_classes)\n",
    "    \n",
    "    for step in range(1001):\n",
    "        x_batch, y_batch, seqlens_batch = get_train_batch(batch_size, X_train_all, y_train_all, seqlens_train_all)\n",
    "        \n",
    "        y_batch_one_hot = one_hot(y_batch.astype(int), vals=num_classes)\n",
    "        \n",
    "        sess.run(train_step, feed_dict={inputs:x_batch, labels:y_batch_one_hot, seqlens:seqlens_batch})\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            summary_train, accuracy_train = sess.run([merged, accuracy], \n",
    "                                                      feed_dict={inputs:x_batch, \n",
    "                                                                 labels:y_batch_one_hot, \n",
    "                                                                 seqlens:seqlens_batch})\n",
    "            print('Accuracy at step {}'.format(step))\n",
    "            print('\\tTrain Set: {:.3f}'.format(accuracy_train))\n",
    "            train_writer.add_summary(summary_train, step)\n",
    "    \n",
    "            summary_test, batch_pred, accuracy_test = sess.run([merged, tf.argmax(final_output,1), accuracy],\n",
    "                                                                feed_dict={inputs:X_test_all, \n",
    "                                                                           labels:y_test_all_one_hot, \n",
    "                                                                           seqlens:seqlens_test_all})\n",
    "            test_writer.add_summary(summary_test, step)\n",
    "    \n",
    "            print('\\tTest Set:  {:.3f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T23:03:26.762807Z",
     "start_time": "2018-12-26T23:03:26.745806Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_back(mat_one_hot, vals=num_classes):\n",
    "    n = np.shape(mat_one_hot)[0]\n",
    "    out = np.zeros(n)\n",
    "    for ii in range(num_classes):\n",
    "        ind = np.where(mat_one_hot[:,ii])\n",
    "        out[ind] = ii\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T23:03:27.171830Z",
     "start_time": "2018-12-26T23:03:26.765807Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hot_back(y_batch_one_hot, vals=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T23:13:56.978853Z",
     "start_time": "2018-12-26T23:13:56.919849Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T23:13:57.374875Z",
     "start_time": "2018-12-26T23:13:57.223867Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T00:42:46.680694Z",
     "start_time": "2018-12-27T00:42:46.493684Z"
    }
   },
   "outputs": [],
   "source": [
    "print_precision_recall_accuracy(batch_pred, y_test_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T07:25:13.497517Z",
     "start_time": "2018-12-28T07:25:13.369510Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_misclassified_data_points(batch_pred, y_test_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T07:22:47.451164Z",
     "start_time": "2018-12-28T07:22:47.419162Z"
    }
   },
   "outputs": [],
   "source": [
    "print_precision_recall_accuracy([0,1,2,3], [0,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
