{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Windowing Procedure and Peak Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing the test data\n",
    "\n",
    "The windowing follows a certain procedure (parameters are variable):\n",
    "\n",
    ">•\tTaking a 1 s block of the data\n",
    "\n",
    ">•\tVarying the block length from 1 s to 5 s with an increment of 200 ms (starting point remains the same for all blocks)\n",
    "\n",
    ">•\tSectioning and feature generation for all blocks\n",
    "\n",
    ">•\tFor each block class probabilities are calculated (ML classifier) \n",
    "\n",
    ">•\tSliding the starting point with an increment of 200 ms and starting again with a 1 s block varying to 5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:53.607978Z",
     "start_time": "2019-01-16T22:41:51.713869Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.widgets import Slider, Button\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import skinematics as skin\n",
    "from scipy.signal import butter, lfilter\n",
    "from IPython.display import clear_output\n",
    "from scipy.ndimage.filters import maximum_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:53.635979Z",
     "start_time": "2019-01-16T22:41:53.610978Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_progress_func(current_num, max_num, prev_prog, add_info=None):\n",
    "    '''\n",
    "    Function to print progress [%] in a loop.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_num : int\n",
    "        Number of the current run in a loop.\n",
    "        \n",
    "    max_num : int\n",
    "        Maximum number of runs in a loop.\n",
    "        \n",
    "    prev_prog : int\n",
    "        Previous progress, to print only if necessary.\n",
    "        \n",
    "    add_info : str\n",
    "        Additional information to print instead of \"Progress\".\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Previous progress, important for next run.\n",
    "    '''\n",
    "    new_prog = int(current_num/max_num*100)\n",
    "    \n",
    "    if new_prog > prev_prog:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if isinstance(add_info, str):\n",
    "            print(add_info + ' {:3d}%'.format(new_prog))\n",
    "        else:\n",
    "            print('Progress: {:3d}%'.format(new_prog))\n",
    "        \n",
    "    return new_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:53.812989Z",
     "start_time": "2019-01-16T22:41:53.639980Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_signal(signal_data, axis=0, rot_angle=90, signals=['Acc','Gyr']):\n",
    "    '''\n",
    "    Function to rotate signals around x, y or z-axis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal_data : dict\n",
    "        Dictionary with the signals in the 'signals' argument as keys.\n",
    "        The signal arrays must have three columns (x, y, z).\n",
    "        \n",
    "    axis : int\n",
    "        Axis for rotation:\n",
    "        0, 1 or 2 --> x, y or z\n",
    "        \n",
    "    rot_angle : int or float\n",
    "        Rotation angle in degree.\n",
    "        \n",
    "    signals : list of strings\n",
    "        Names of the signals, which shall be considered for rotation (e.g. ['Acc','Gyr']).\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary with rotated selected signals.\n",
    "    (Same structure as input signal dictionary.)\n",
    "\n",
    "    '''\n",
    "    # if no signals are given as keys, select all keys of the input dictionary\n",
    "    if signals is None:\n",
    "        signals = [*signal_data]\n",
    "    \n",
    "    # create rotation matrix\n",
    "    R = skin.rotmat.R(axis=axis, angle=rot_angle)\n",
    "    \n",
    "    # dictionary for rotated data\n",
    "    rot_signal_data = {}\n",
    "    \n",
    "    # rotate the signals\n",
    "    for sig in signals: \n",
    "        rot_signal_data[sig] = (R @ signal_data[sig].T).T\n",
    "        \n",
    "    return rot_signal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:53.968998Z",
     "start_time": "2019-01-16T22:41:53.818990Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise_to_signal(signal_data, target_snr_db=20, signals=['Acc','Gyr'], signal_orientations=['x','y','z']):\n",
    "    '''\n",
    "    Function to add Additive White Gaussian Noise (AWGN) to all signals with a defined SNR.\n",
    "    \n",
    "    Used formulas:\n",
    "    SNR = P_signal / P_noise\n",
    "    SNR_db = 10 * log10(P_signal / P_noise)\n",
    "    SNR_db = P_signal_db - P_noise_db\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal_data : dict\n",
    "        Dictionary with the signals in the 'signals' argument as keys.\n",
    "        \n",
    "    target_snr_db : int or float\n",
    "        Target signal to noise ration in db.\n",
    "        \n",
    "    signals : list of strings\n",
    "        Names of the signals, which shall be considered for rotation (e.g. ['Acc','Gyr']).\n",
    "        \n",
    "    signal_orientations : list of strings\n",
    "        Orientations of the signals (e.g. ['x','y','z']).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary with noisy signals.\n",
    "    (Same structure as input signal dictionary.)\n",
    "\n",
    "    '''\n",
    "    # if no signals are given as keys, select all keys of the input dictionary\n",
    "    if signals is None:\n",
    "        signals = [*signal_data]\n",
    "    \n",
    "    # dictionary for noisy data\n",
    "    noisy_signal_data = {}\n",
    "    \n",
    "    # adding noise using target SNR\n",
    "    for sig in signals:\n",
    "        \n",
    "        # fill in old values\n",
    "        noisy_signal_data[sig] = np.zeros(np.shape(signal_data[sig]))\n",
    "        \n",
    "        for ii in range(len(signal_orientations)):\n",
    "            \n",
    "            # get power of the signal [watts] (with removed offset)\n",
    "            P_signal_watts = (signal_data[sig][:,ii]-np.mean(signal_data[sig][:,ii])) ** 2\n",
    "            P_signal_mean_watts = np.mean(P_signal_watts) # get mean\n",
    "            P_signal_mean_db = 10 * np.log10(P_signal_mean_watts) # convert to db\n",
    "            \n",
    "            P_noise_mean_db = P_signal_mean_db - target_snr_db # get corresponding noise power\n",
    "            P_noise_mean_watts = 10 ** (P_noise_mean_db/10) # convert from db to watts\n",
    "            noise_mean_std = np.sqrt(P_noise_mean_watts) # std of noise (P_noise_mean_watts is variance)\n",
    "            \n",
    "            # generate sample of white noise (power = variance = P_noise_mean_watts)\n",
    "            noise = np.random.normal(0, noise_mean_std, len(signal_data[sig][:,ii]))\n",
    "            \n",
    "            # add noise to original signal\n",
    "            noisy_signal_data[sig][:,ii] = signal_data[sig][:,ii] + noise\n",
    "\n",
    "    return noisy_signal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:54.105006Z",
     "start_time": "2019-01-16T22:41:53.972999Z"
    }
   },
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    '''\n",
    "    Function to get filter coefficients for butterworth filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cutoff : int or float\n",
    "        Cutoff-frequency of the applied filter.\n",
    "    \n",
    "    fs : int or float\n",
    "        Sampling rate in Hz.\n",
    "    \n",
    "    order : int\n",
    "        Order of the applied filter.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Filter coefficients for butterworth filter (a, b).\n",
    "    '''\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    '''\n",
    "    Filter data with butterworth filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array or matrix like\n",
    "        N-dimensional input array (if matrix --> one signal per column).\n",
    "    \n",
    "    cutoff : int or float\n",
    "        Cutoff-frequency of the applied filter.\n",
    "    \n",
    "    fs : int or float\n",
    "        Sampling rate in Hz.\n",
    "    \n",
    "    order : int\n",
    "        Order of the applied filter.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Filtered data (matrix or array-like)\n",
    "    '''\n",
    "    \n",
    "    b, a = butter_lowpass(cutoff, fs, order=order) # from scipy.signal\n",
    "    \n",
    "    # filter data along one-dimension\n",
    "    y = lfilter(b, a, data, axis=0) # from scipy.signal\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:54.268016Z",
     "start_time": "2019-01-16T22:41:54.108006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sensor_data(in_file, \n",
    "                    signals=['Acc','Gyr','Mag'], \n",
    "                    sampling_rate=256, \n",
    "                    start_time=None, \n",
    "                    stop_time=None, \n",
    "                    skip_rows=0, \n",
    "                    sep=',',\n",
    "                    return_time_array=True,\n",
    "                    add_info='no info'):\n",
    "    '''\n",
    "    Function to read sensor data from a file, in order to return data from selected sensors and time range.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_file: string\n",
    "        Directory and file name of data (e.g. 'Subject_01/subject01.csv').\n",
    "    \n",
    "    signals: list of stings\n",
    "        Sensor signal abbreviations (have to be equal to the first letters of the data column names!).\n",
    "    \n",
    "    sampling_rate: int or float\n",
    "        Sampling rate of the measured signals in Hz.\n",
    "    \n",
    "    start_time : int or float\n",
    "        Start time for selecting data in sec (if None --> start from beginning).\n",
    "    \n",
    "    stop_time : int or float\n",
    "        Stop time for selecting data in sec (if None --> until end of data).\n",
    "    \n",
    "    csv_skiprows : int\n",
    "        Number of rows to skip for pandas read_csv() function.\n",
    "    \n",
    "    csv_separator : char\n",
    "        Seperator for pandas read_csv() function.\n",
    "    \n",
    "    return_time_array : boolean\n",
    "        If True: out dict has an item (np.array) containing the time (key: \"time\").\n",
    "    \n",
    "    add_info: string\n",
    "        Additional info to plot if error occurs.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary with selected data and time array [s]\n",
    "    '''\n",
    "    \n",
    "    data = pd.read_csv(in_file, skiprows=skip_rows, sep=sep)\n",
    "    \n",
    "    num_steps = np.shape(data.values)[0] # total number of data points\n",
    "    \n",
    "    if start_time is None:\n",
    "        start_index = 0\n",
    "    else:\n",
    "        start_index = round(start_time * sampling_rate)\n",
    "        \n",
    "    if stop_time is None:\n",
    "        stop_index = num_steps\n",
    "    else:\n",
    "        stop_index = round(stop_time * sampling_rate)\n",
    "        \n",
    "    if start_index < 0 or stop_index > num_steps or start_index >= stop_index:\n",
    "        print('Error at selecting data from given time range. (' + add_info + ')')\n",
    "        return {}\n",
    "        \n",
    "    data_dict = {}\n",
    "    for signal in signals:\n",
    "        data_dict[signal] = data.filter(regex=signal+'*').values[start_index:stop_index]\n",
    "    \n",
    "    if return_time_array:\n",
    "        data_dict['time'] = np.arange(num_steps)[start_index:stop_index] / sampling_rate\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:54.882051Z",
     "start_time": "2019-01-16T22:41:54.272016Z"
    }
   },
   "outputs": [],
   "source": [
    "def signal_windowing_via_indices(test_subject_path,\n",
    "                                 number_sections=10,\n",
    "                                 sig_names=['Acc','Gyr'],\n",
    "                                 signal_orientations=['x','y','z'],\n",
    "                                 sampling_rate=256,\n",
    "                                 cutoff=10,\n",
    "                                 order=6,\n",
    "                                 win_start_inc=0.2,\n",
    "                                 win_stretch_inc=0.2,\n",
    "                                 win_min_len=1,\n",
    "                                 win_max_len=5,\n",
    "                                 win_start=0,\n",
    "                                 win_last_start=None,\n",
    "                                 print_progress=True,\n",
    "                                 progress_info='Generate feature map...',\n",
    "                                 rot_axis=0,\n",
    "                                 rot_angle=0,\n",
    "                                 add_noise=False,\n",
    "                                 target_snr_db=20,\n",
    "                                 csv_skiprows=0,\n",
    "                                 csv_separator=','):\n",
    "    '''\n",
    "    This function applies a defined windowing procedure in order to split a signal \n",
    "    into different sections, which can be then taken as features for machine learning.\n",
    "    The different section values are determined by taking the index in the middle of\n",
    "    the corresponding section.\n",
    "    In order to avoid extreme outliers a butterworth filter is used before sectioning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_subject_path : str\n",
    "        Path to the csv-file of the test subject data.\n",
    "        \n",
    "    number_sections: int\n",
    "        Number of sections to split each window.\n",
    "        \n",
    "    sig_names : list of strings\n",
    "        Signal names, used as keys for signal dictionaries.\n",
    "        \n",
    "    signal_orientations : list of strings\n",
    "        Orientations of the signals (e.g. ['x','y','z']).\n",
    "        \n",
    "    sampling_rate : int or float\n",
    "        Sampling rate of the signals.\n",
    "        \n",
    "    cutoff : int or float\n",
    "        Cutoff frequency of the butterworh filter.\n",
    "        \n",
    "    order : int\n",
    "        Order of the butterworth filter.\n",
    "        \n",
    "    win_start_inc : int or float\n",
    "        Start increment for the window [s].\n",
    "        \n",
    "    win_stretch_inc : int or float\n",
    "        Stretch increment for the window [s].\n",
    "    \n",
    "    win_min_len : int or float\n",
    "        Minimum window length [s].\n",
    "    \n",
    "    win_max_len : int or float\n",
    "        Maximum window length [s].\n",
    "    \n",
    "    win_start : int or float\n",
    "        Start time of the window [s].\n",
    "    \n",
    "    win_last_start : int or float or None\n",
    "        Last start time of the window [s].\n",
    "        If None, set to time where the minimum window length just fits into the sensor data.\n",
    "    \n",
    "    print_progress : boolean\n",
    "        If True --> print progress at feature generation.\n",
    "    \n",
    "    progress_info : str\n",
    "        Information to print with progress.\n",
    "        \n",
    "    rot_axis : int or list of int\n",
    "        Axis for rotation:\n",
    "        0, 1 or 2 --> x, y or z\n",
    "        --> if list: sequence of rotations\n",
    "        (Length of list has to match with the length of rot_angle,\n",
    "        otherwise the shorter list of the two is taken and all other values are omitted.)\n",
    "        \n",
    "    rot_angle : int or float or list of int or float\n",
    "        Rotation angle in degree.\n",
    "        --> if list: sequence of rotations\n",
    "        (Length of list has to match with the length of rot_axis,\n",
    "        otherwise the shorter list of the two is taken and all other values are omitted.)\n",
    "        \n",
    "    add_noise : boolean\n",
    "        If True --> noise is added to signals.\n",
    "        \n",
    "    target_snr_db : int or float\n",
    "        Signal to noise ratio in db for the generated noisy signals.\n",
    "    \n",
    "    csv_skiprows : int\n",
    "        Number of rows to skip for pandas read_csv() function.\n",
    "    \n",
    "    csv_separator : char\n",
    "        Seperator for pandas read_csv() function.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list[0] : numpy.ndarray\n",
    "            Matrix with sectioned signal data.\n",
    "                (Number of columns = number of features)\n",
    "                (Number of rows = number of data points)\n",
    "        \n",
    "        list[1] : numpy.ndarray\n",
    "            Array with all possible window start points for the choosen parameters [s].\n",
    "        \n",
    "        list[2] : numpy.ndarray\n",
    "            Array with all possible window lengths for the choosen parameters [s].\n",
    "        \n",
    "        list[3] : int\n",
    "            Length of the original signals (number of indices).\n",
    "    '''\n",
    "\n",
    "\n",
    "    # get data from selected file\n",
    "    sensor_data = get_sensor_data(in_file=test_subject_path,\n",
    "                                  signals=sig_names,\n",
    "                                  sampling_rate=sampling_rate,\n",
    "                                  skip_rows=csv_skiprows,\n",
    "                                  sep=csv_separator)\n",
    "    \n",
    "    # rotate the signals\n",
    "    if not isinstance(rot_axis, list): # if not list --> make list\n",
    "        rot_axis = [rot_axis]\n",
    "    if not isinstance(rot_angle, list): # if not list --> make list\n",
    "        rot_angle = [rot_angle]\n",
    "    # going through all rotation axes and rotation angles\n",
    "    for current_rot_axis, current_rot_angle in zip(rot_axis, rot_angle):\n",
    "        # apply rotation only if rotation angle is not zero\n",
    "        if current_rot_angle != 0:\n",
    "            sensor_data = rotate_signal(sensor_data, \n",
    "                                        axis=current_rot_axis, \n",
    "                                        rot_angle=current_rot_angle, \n",
    "                                        signals=sig_names)\n",
    "\n",
    "    # add noise to signal if corresponding parameter is True\n",
    "    if add_noise is True:\n",
    "        sensor_data = add_noise_to_signal(sensor_data,\n",
    "                                          target_snr_db=target_snr_db, \n",
    "                                          signals=sig_names, \n",
    "                                          signal_orientations=signal_orientations)\n",
    "\n",
    "    # filter data with butterworth filter and save to new dictionary\n",
    "    sensor_data_filt = {}\n",
    "    for signal in sig_names:\n",
    "        sensor_data_filt[signal] = butter_lowpass_filter(sensor_data[signal], \n",
    "                                                         cutoff=cutoff, \n",
    "                                                         fs=sampling_rate, \n",
    "                                                         order=order)\n",
    "\n",
    "    # signal length: all sensor data must have same length --> Acc, Gyr, ...\n",
    "    # --> but to ensure that indices are not out of range in case of wrong input data\n",
    "    # let's take the smallest stop index of the different signals\n",
    "    signal_len = float('inf')\n",
    "    for sig in sig_names:\n",
    "        if np.shape(sensor_data_filt[sig])[0] < signal_len:\n",
    "            signal_len = np.shape(sensor_data_filt[sig])[0]\n",
    "\n",
    "    # last window start is None--> set to time where the minimum window length just fits into the sensor data\n",
    "    if win_last_start is None:\n",
    "        win_last_start = signal_len/sampling_rate - win_min_len\n",
    "    \n",
    "    # array with all possible window start points\n",
    "    all_window_start_points = np.arange(win_start, win_last_start, win_start_inc)\n",
    "    # include win_last_start if the last start point plus the increment is equal to that value (adding end point)\n",
    "    # (round due to small discrepancy)\n",
    "    if round(all_window_start_points[-1] + win_start_inc, 5) == win_last_start:\n",
    "        all_window_start_points = np.append(all_window_start_points, win_last_start)\n",
    "    \n",
    "    # array with all possible window lengths\n",
    "    all_window_lengths = np.arange(win_min_len, win_max_len, win_stretch_inc)\n",
    "    # include win_max_len if the max window length plus the increment is equal to that value (adding end point)\n",
    "    # (round due to small discrepancy)\n",
    "    if round(all_window_lengths[-1] + win_stretch_inc, 5) == win_max_len:\n",
    "        all_window_lengths = np.append(all_window_lengths, win_max_len)\n",
    "    \n",
    "    # number of different window start points\n",
    "    num_start_points = len(all_window_start_points)\n",
    "    \n",
    "    # number of different window sizes\n",
    "    num_win_sizes = len(all_window_lengths)\n",
    "\n",
    "    # matrix with all generated features (number_sections*6 = number of features --> Acc, Gyr (x,y,z))\n",
    "    feature_map = np.zeros([num_start_points * num_win_sizes, number_sections*6])\n",
    "    \n",
    "    # counter for current position (row) in the feature map\n",
    "    count = 0\n",
    "    \n",
    "    # variables for progress printing\n",
    "    max_count = len(feature_map)\n",
    "    prev_progress = 0 # previous progress\n",
    "\n",
    "    # going through all window start points\n",
    "    for ii, win_pos in enumerate(all_window_start_points):\n",
    "\n",
    "        # going through all window lengths\n",
    "        for jj, win_len in enumerate(all_window_lengths):\n",
    "\n",
    "            # calculate start and stop index (type: float --> conversion to int happens afterwards)\n",
    "            start_index = win_pos * sampling_rate\n",
    "            stop_index = start_index + (win_len * sampling_rate)\n",
    "\n",
    "            # check if stop index is out of range\n",
    "            if stop_index >= signal_len:\n",
    "                stop_index = signal_len-1 # set equal to last index\n",
    "\n",
    "            # get indices of the sections\n",
    "            section_indices, step = np.linspace(start_index, stop_index, number_sections, endpoint=False, retstep=True)\n",
    "\n",
    "            #  + step/2 in order to get the indices in the middle of the sections\n",
    "            section_indices = (section_indices + step/2).round().astype(int)\n",
    "\n",
    "            # putting the feature map together\n",
    "            #feature_map[count,:] = np.concatenate((sensor_data_filt[sig_names[0]][section_indices,:].transpose(), \n",
    "            #                                       sensor_data_filt[sig_names[1]][section_indices,:].transpose())).flatten().reshape(1, -1)\n",
    "            feature_map[count,:] = np.concatenate([sensor_data_filt[sig][section_indices,:].transpose().flatten() for sig in sig_names])\n",
    "\n",
    "            count += 1\n",
    "        \n",
    "        # print progress of feauture map generation\n",
    "        if print_progress:\n",
    "            prev_progress = print_progress_func(count, max_count, prev_progress, add_info=progress_info)\n",
    "    \n",
    "    return [feature_map, all_window_start_points, all_window_lengths, signal_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:55.077062Z",
     "start_time": "2019-01-16T22:41:54.885051Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_prob_map_peaks(prob_matrix,\n",
    "                          win_start_inc,\n",
    "                          num_win_sizes,\n",
    "                          threshold_prob=0.5, \n",
    "                          footprint_length=1.5):\n",
    "    '''\n",
    "    Function to detect the local peaks of a probability map.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prob_matrix : 2d-array\n",
    "        Matrix with predicted probabilities.\n",
    "        \n",
    "    threshold_prob : int or float\n",
    "        Find only peaks with a minimum probability (threshold).\n",
    "        \n",
    "    footprint_length : int or float\n",
    "        Length of the footprint for the maximum_filter in order to find peaks [s].\n",
    "        \n",
    "    win_start_inc : int or float\n",
    "        Window start increment [s].\n",
    "        \n",
    "    num_win_sizes : int\n",
    "        Number of different window sizes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array \n",
    "        array[0] ... peak time indices\n",
    "        array[1] ... peak window length indices\n",
    "        e.g. ([[ 390, 723, 1331, ...], [4, 4, 10, ...]], dtype=int64)\n",
    "    '''\n",
    "    \n",
    "    # length and height of the footprint for the maximum_filter (see below)\n",
    "    footprint_length_indices = int(footprint_length / win_start_inc)\n",
    "    footprint_height = num_win_sizes * 2  # take twice the number of all window sizes for footprint height\n",
    "    \n",
    "    footprint=np.ones((footprint_length_indices,footprint_height))\n",
    "    \n",
    "    # applying a maximum filter and generating a boolean map for local maxima\n",
    "    local_max = maximum_filter(prob_matrix, footprint=footprint)==prob_matrix\n",
    "    \n",
    "    # removing all maxima below the threshold\n",
    "    local_max = (prob_matrix>=threshold_prob) & local_max\n",
    "    \n",
    "    # check if there are several points with the same probability at one local maxima (within footprint length)\n",
    "    #   --> remove them, otherwise we get more than one local maxima\n",
    "    peak_indices_check = np.argwhere(local_max)\n",
    "    if len(peak_indices_check) > 1:\n",
    "        for ii in range(len(peak_indices_check)-1):\n",
    "            row_ind, col_ind = peak_indices_check[ii]\n",
    "            row_ind_next, col_ind_next = peak_indices_check[ii+1]\n",
    "            if row_ind_next-row_ind <= footprint_length_indices/2:\n",
    "                local_max[row_ind,col_ind] = False\n",
    "    \n",
    "    # get the maxima indices of the probability map\n",
    "    peak_indices = np.argwhere(local_max).transpose()\n",
    "    \n",
    "    return peak_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:55.387080Z",
     "start_time": "2019-01-16T22:41:55.085062Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_peaks(peak_ind,\n",
    "                   prob_matrix,\n",
    "                   win_start_inc,\n",
    "                   exercise_abbrs_peak_eval,\n",
    "                   max_time_between_peaks=10,\n",
    "                   min_peaks_per_block=3):\n",
    "    '''\n",
    "    Function to evaluate the detected peaks.\n",
    "    (see function detect_prob_map_peaks(prob_matrix))\n",
    "    \n",
    "    --> assign peaks to repetition blocks with min two repetitions\n",
    "    --> if blocks are overlapping, keep only the block with the highest predicted probabilities (sum)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    peak_ind : dict\n",
    "        Exercise-abbreviations as keys (e.g. 'RF', 'RO', ...)\n",
    "        --> values: 2d-array \n",
    "        array[0] ... peak time indices\n",
    "        array[1] ... peak window length indices\n",
    "        e.g. ([[ 390, 723, 1331, ...], [4, 4, 10, ...]], dtype=int64)\n",
    "        \n",
    "    prob_matrix : dict\n",
    "        Exercise-abbreviations as keys (e.g. 'RF', 'RO', ...)\n",
    "        --> values: 2d-array \n",
    "        Matrices with predicted probabilities.\n",
    "        \n",
    "    win_start_inc : int or float\n",
    "        Window start increment.\n",
    "        \n",
    "    exercise_abbrs_peak_eval : list of strings\n",
    "        Exercises considered for peak evaluation.\n",
    "        \n",
    "    max_time_between_peaks : int or float\n",
    "        Maximum time between two peaks in the same block [s].\n",
    "        \n",
    "    min_peaks_per_block : int\n",
    "        Minimum number of peaks per block.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with exercise abbreviations as keys --> repetition blocks\n",
    "        \n",
    "        Example: rep_blocks['RF'][0] (np.narray)  (first block of exercise 'RF'):\n",
    "        [[4121, 9],\n",
    "         [4135, 11],\n",
    "         [4150, 11],\n",
    "         [4166, 9],\n",
    "         [4179, 10],\n",
    "         [4193, 10],\n",
    "         [4207, 10],\n",
    "         [4221, 12],\n",
    "         [4236, 13],\n",
    "         [4251, 13]]\n",
    "           --> 1st column: indices corresponding to horizontal axis (window start position)\n",
    "           --> 2nd column: indices corresponding to vertical axis (window stretching)\n",
    "           --> 10 rows --> 10 repetitions in this block\n",
    "    '''\n",
    "    \n",
    "    # define the maximum time between two peaks in a block\n",
    "    max_ind_between_peaks = int(max_time_between_peaks / win_start_inc)\n",
    "    \n",
    "    exercise_abbrs_peak_eval = [*peak_ind]\n",
    "    \n",
    "    # assign peaks to repetition blocks\n",
    "    rep_blocks = {}\n",
    "    for ex in exercise_abbrs_peak_eval:\n",
    "        rep_blocks[ex] = []\n",
    "        new_block = True # remember if current peak belongs to a new block\n",
    "        \n",
    "        # going through all time indices of the peaks of the current exercise\n",
    "        for current_peak_time_ind, current_peak_win_ind in zip(peak_ind[ex][0], peak_ind[ex][1]):\n",
    "            \n",
    "            # if the current time index belongs to a new block --> append new block\n",
    "            if new_block is True:\n",
    "                rep_blocks[ex].append(np.array([[current_peak_time_ind, current_peak_win_ind]]))\n",
    "                new_block = False\n",
    "            \n",
    "            # check if previous peak is within acceptable temporal distance in order to belong to the same block\n",
    "            elif current_peak_time_ind - rep_blocks[ex][-1][-1,0] <= max_ind_between_peaks:\n",
    "                # append (stack) the current peak to the last block\n",
    "                rep_blocks[ex][-1] = np.vstack((rep_blocks[ex][-1], \n",
    "                                               np.array([[current_peak_time_ind, current_peak_win_ind]])))\n",
    "                \n",
    "            # append a new block\n",
    "            else:\n",
    "                rep_blocks[ex].append(np.array([[current_peak_time_ind, current_peak_win_ind]]))\n",
    "    \n",
    "    \n",
    "    # check if the repetition blocks have a minimum number of peaks (min_peaks_per_block)\n",
    "    valid_rep_blocks = {}\n",
    "    for ex in exercise_abbrs_peak_eval:\n",
    "        valid_rep_blocks[ex] = []\n",
    "        # going through all blocks of the current exercise\n",
    "        for rep_block in rep_blocks[ex]:\n",
    "            # retain the block only if there is a minimum number of peaks\n",
    "            if np.shape(rep_block)[0] >= min_peaks_per_block:\n",
    "                valid_rep_blocks[ex].append(rep_block)\n",
    "    \n",
    "    \n",
    "    # if blocks are overlapping --> retain only the block with the highest predicted probabilities (sum)\n",
    "    #    --> the more peaks in the block, the higher the sum of probabilities (in general)\n",
    "    blocks_to_remove = []\n",
    "    # check all combinations of two exercises\n",
    "    for ex1, ex2 in itertools.combinations(exercise_abbrs_peak_eval, 2):\n",
    "        for ii in range(len(valid_rep_blocks[ex1])):\n",
    "            for jj in range(len(valid_rep_blocks[ex2])):\n",
    "                start_1 = valid_rep_blocks[ex1][ii][0,0] # time index of the first peak in the current block 1\n",
    "                stop_1 = valid_rep_blocks[ex1][ii][-1,0] # time index of the last peak in the current block 1\n",
    "                start_2 = valid_rep_blocks[ex2][jj][0,0] # time index of the first peak in the current block 2\n",
    "                stop_2 = valid_rep_blocks[ex2][jj][-1,0] # time index of the last peak in the current block 2\n",
    "\n",
    "                # check if the two blocks overlap\n",
    "                if (start_1 >= start_2 and start_1 <= stop_2) or (stop_1 >= start_2 and stop_1 <= stop_2) \\\n",
    "                or (start_2 >= start_1 and start_2 <= stop_1) or (stop_2 >= start_1 and stop_2 <= stop_1):\n",
    "\n",
    "                    # selecet the corresponding probability values of prob_matrix and sum them up\n",
    "                    sum_prob_block_1 = prob_matrix[ex1][rep_blocks[ex1][ii][:,0], \n",
    "                                                        rep_blocks[ex1][ii][:,1]].sum()\n",
    "\n",
    "                    sum_prob_block_2 = prob_matrix[ex2][rep_blocks[ex2][jj][:,0], \n",
    "                                                        rep_blocks[ex2][jj][:,1]].sum()\n",
    "\n",
    "                    # compare the sum of the probabilities of the two blocks\n",
    "                    if sum_prob_block_1 < sum_prob_block_2:\n",
    "                        blocks_to_remove.append([ex1, ii])\n",
    "                    else:\n",
    "                        blocks_to_remove.append([ex2, jj])\n",
    "    \n",
    "    # ensure that there are no duplicates in the nested list\n",
    "    blocks_to_remove_unique = []\n",
    "    for sublist in blocks_to_remove:\n",
    "        if sublist not in blocks_to_remove_unique:\n",
    "            blocks_to_remove_unique.append(sublist)\n",
    "    \n",
    "    # by removing the blocks take the reversed sorted order of the block index\n",
    "    #    --> so it is possible to remove all blocks without \"refreshing\" the indices\n",
    "    #        (if one block is removed, higher indices of all other blocks are changing)\n",
    "    for ex, block_ind in sorted(blocks_to_remove_unique, key=lambda x: x[1])[::-1]:\n",
    "        valid_rep_blocks[ex].pop(block_ind)\n",
    "        \n",
    "    return valid_rep_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:55.582091Z",
     "start_time": "2019-01-16T22:41:55.390080Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_time_format(min_sec, sampling_rate=None, time_offset=0, max_index=None, convert_to_s=False):\n",
    "    '''\n",
    "    Function converts a string with the time format 'min:sec' (e.g. 5:17.2)\n",
    "    to a corresponding index, considering the sampling rate.\n",
    "    If index would be negative, 0 is returned.\n",
    "    If convert_to_s is True --> convert to seconds instead.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_sec : string\n",
    "        Time data, defined format: 'min:sec'\n",
    "    \n",
    "    sampling_rate : float or int\n",
    "        Sampling rate for the index calculation. [Hz]\n",
    "        \n",
    "    time_offset : float of int\n",
    "        Time offset, considered at the index calculation. [s]\n",
    "        \n",
    "    max_index : int\n",
    "        Maximum valid index.\n",
    "        If provided and calculated index is out of range,\n",
    "        max_index is returned instead.\n",
    "        \n",
    "    convert_to_s : boolean\n",
    "        If True --> convert to seconds.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int or float\n",
    "        Corresponding index or value [s] to parameter 'min_max'.\n",
    "    '''\n",
    "    \n",
    "    # split time string and convert to float\n",
    "    minutes = float(min_sec.split(':')[0])\n",
    "    seconds = float(min_sec.split(':')[1])\n",
    "    \n",
    "    # start and stop time in seconds\n",
    "    time_s = minutes*60 + seconds + time_offset\n",
    "    \n",
    "    if convert_to_s is True:\n",
    "        return time_s\n",
    "    \n",
    "    # get corresponding index\n",
    "    index = round(time_s * sampling_rate)\n",
    "    \n",
    "    # ensure that index is not below 0\n",
    "    if index < 0:\n",
    "        index = 0\n",
    "    \n",
    "    # ensure that index is in valid range if max index is given\n",
    "    if max_index is not None and index > max_index:\n",
    "        index = max_index\n",
    "            \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:55.738100Z",
     "start_time": "2019-01-16T22:41:55.588091Z"
    }
   },
   "outputs": [],
   "source": [
    "def indices_to_time(start_index, stop_index, win_start_inc):\n",
    "    '''\n",
    "    Function convert indices to time string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_index : int\n",
    "        \n",
    "    stop_index : int\n",
    "    \n",
    "    win_start_inc : int or float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        String with start and stop time (e.g. '14:39.6 - 15:19.4').\n",
    "    '''\n",
    "    \n",
    "    start_time_text = '{0:02}:{1:04.1f}'.format(int(start_index*win_start_inc/60), \n",
    "                                               (start_index*win_start_inc)%60)\n",
    "    stop_time_text = '{0:02}:{1:04.1f}'.format(int(stop_index*win_start_inc/60), \n",
    "                                               (stop_index*win_start_inc)%60)\n",
    "    return start_time_text + ' - ' + stop_time_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:55.910109Z",
     "start_time": "2019-01-16T22:41:55.743100Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_prediction_matrix(pred_probs,\n",
    "                           exercise_abbrs,\n",
    "                           num_start_points,\n",
    "                           num_win_sizes,\n",
    "                           all_window_start_points,\n",
    "                           all_window_lengths):\n",
    "    \n",
    "    '''\n",
    "    Function to write the predicted probabilities into a dictionary \n",
    "    with prediction matrices as elements for each exercise (key).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_probs : numpy.ndarray\n",
    "        Matrix with all predicted probabilities.\n",
    "        (Number of rows: number of data points)\n",
    "        (Number of columns: number of labels)\n",
    "        \n",
    "    exercise_abbrs : list of strings\n",
    "        Abbreviations of exercises.\n",
    "        \n",
    "    num_start_points : int\n",
    "        Number of window start points.\n",
    "        \n",
    "    num_win_sizes : int\n",
    "        Number of different window sizes.\n",
    "        \n",
    "    all_window_start_points : array\n",
    "        Array with all window start points.\n",
    "        \n",
    "    all_window_lengths : array\n",
    "        Array with all different window lengths.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        String with start and stop time (e.g. '14:39.6 - 15:19.4').\n",
    "    '''\n",
    "    \n",
    "    count = 0 # counter for the current row of the matrix with the predicted probabilities\n",
    "\n",
    "    # dictionary with matrices to save predicted values for all classes\n",
    "    prob_matrix_dict = {}\n",
    "    for ex in exercise_abbrs:\n",
    "        prob_matrix_dict[ex] = np.zeros([num_start_points, num_win_sizes])\n",
    "\n",
    "    # going through all window start points\n",
    "    for ii, win_pos in enumerate(all_window_start_points):\n",
    "\n",
    "        # going through all window lengths \n",
    "        for jj, win_len in enumerate(all_window_lengths):\n",
    "\n",
    "            for kk, ex in enumerate(exercise_abbrs):\n",
    "                prob_matrix_dict[ex][ii,jj] = pred_probs[count,kk]\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "    return prob_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:41:59.021287Z",
     "start_time": "2019-01-16T22:41:55.917110Z"
    }
   },
   "outputs": [],
   "source": [
    "class PhysioData_WindowingProcedure():\n",
    "    '''\n",
    "    Class for feature generation according to a certain windowing procedure.\n",
    "    There are various selectable options --> see Parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_subject_dir : string\n",
    "        Directory to the csv-file of the test subject data.\n",
    "    \n",
    "    test_subject_file : string\n",
    "        Name of the csv-file.\n",
    "        \n",
    "    number_sections: int\n",
    "        Number of sections to split each window.\n",
    "        \n",
    "    sig_names : list of strings\n",
    "        Signal names, used as keys for signal dictionaries.\n",
    "        \n",
    "    signal_orientations : list of strings\n",
    "        Orientations of the signals (e.g. ['x','y','z']).\n",
    "        \n",
    "    sampling_rate : int or float\n",
    "        Sampling rate of the signals.\n",
    "        \n",
    "    cutoff : int or float\n",
    "        Cutoff frequency of the butterworh filter.\n",
    "        \n",
    "    order : int\n",
    "        Order of the butterworth filter.\n",
    "        \n",
    "    win_start_inc : int or float\n",
    "        Start increment for the window [s].\n",
    "        \n",
    "    win_stretch_inc : int or float\n",
    "        Stretch increment for the window [s].\n",
    "    \n",
    "    win_min_len : int or float\n",
    "        Minimum window length [s].\n",
    "    \n",
    "    win_max_len : int or float\n",
    "        Maximum window length [s].\n",
    "    \n",
    "    win_start_min_sec : string\n",
    "        Start time of the window ['min:sec'] (e.g. '05:30.0').\n",
    "    \n",
    "    win_last_start_min_sec : string or None\n",
    "        Last start time of the window ['min:sec'] (e.g. '10:30.0').\n",
    "        If None, set to time where the minimum window length just fits into the sensor data.\n",
    "    \n",
    "    print_progress : boolean\n",
    "        If True --> print progress at feature generation.\n",
    "    \n",
    "    progress_info : str\n",
    "        Information to print with progress.\n",
    "        \n",
    "    rot_axis : int or list of int\n",
    "        Axis for rotation:\n",
    "        0, 1 or 2 --> x, y or z\n",
    "        --> if list: sequence of rotations\n",
    "        (Length of list has to match with the length of rot_angle,\n",
    "        otherwise the shorter list of the two is taken and all other values are omitted.)\n",
    "        \n",
    "    rot_angle : int or float or list of int or float\n",
    "        Rotation angle in degree.\n",
    "        --> if list: sequence of rotations\n",
    "        (Length of list has to match with the length of rot_axis,\n",
    "        otherwise the shorter list of the two is taken and all other values are omitted.)\n",
    "        \n",
    "    add_noise : boolean\n",
    "        If True --> noise is added to signals.\n",
    "        \n",
    "    target_snr_db : int or float\n",
    "        Signal to noise ratio in db for the generated noisy signals.\n",
    "    \n",
    "    csv_skiprows : int\n",
    "        Number of rows to skip for pandas read_csv() function.\n",
    "    \n",
    "    csv_separator : char\n",
    "        Seperator for pandas read_csv() function.\n",
    "        \n",
    "    exercise_abbrs : list of strings\n",
    "        Exercise abbreviations (sequence matters).\n",
    "        \n",
    "    exercise_abbrs_peak_eval : list of strings\n",
    "        Exercises to consider for peak evaluation (e.g. omit non-exercise).\n",
    "        \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    get_feature_map()\n",
    "        Returns the feature map.\n",
    "        \n",
    "    evaluate_probability_matrix()\n",
    "        Method to evaluate a probability matrix.\n",
    "        --> Parameters: See docstring of method.\n",
    "        \n",
    "    print_rep_blocks()\n",
    "        (!) Call this method only after evaluate_probability_matrix().\n",
    "        Method to print the found repetition blocks of each exercise \n",
    "        with time range and number of repetitons.\n",
    "        --> Parameters: See docstring of method.\n",
    "        \n",
    "    plot_probability_matrices_and_peaks()\n",
    "        (!) Call this method only after evaluate_probability_matrix()\n",
    "        Method to plot the probability matrix as well as\n",
    "        the evaluated peaks (repetitions).\n",
    "        --> Parameters: See docstring of method.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 test_subject_dir  = r'E:\\Physio_Data\\Subject_01',\n",
    "                 test_subject_file = 'subject01.csv',\n",
    "                 number_sections=10,\n",
    "                 signal_abbrs=['Acc','Gyr'],\n",
    "                 signal_orientations=['x','y','z'],\n",
    "                 sampling_rate=256,\n",
    "                 cutoff=10,\n",
    "                 order=6,\n",
    "                 win_start_inc=0.2,\n",
    "                 win_stretch_inc=0.2,\n",
    "                 win_min_len=1,\n",
    "                 win_max_len=5,\n",
    "                 win_start_min_sec='00:00.0',\n",
    "                 win_last_start_min_sec=None,\n",
    "                 print_progress=True,\n",
    "                 progress_info='Generate feature map...',\n",
    "                 rot_axis=0,\n",
    "                 rot_angle=0,\n",
    "                 add_noise=False,\n",
    "                 target_snr_db=20,\n",
    "                 csv_skiprows=0,\n",
    "                 csv_separator=',',\n",
    "                 exercise_abbrs=['RF','RO','RS','LR','BC','TC','MP','SA','P1','P2','NE'],\n",
    "                 exercise_abbrs_peak_eval = ['RF','RO','RS','LR','BC','TC','MP','SA','P1','P2']):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        --> See class docstring.\n",
    "        \"\"\"\n",
    "        \n",
    "        # convert window start position and last window start position to value in seconds\n",
    "        self.win_start = convert_time_format(win_start_min_sec, convert_to_s=True)\n",
    "        if win_last_start_min_sec:\n",
    "            self.win_last_start = convert_time_format(win_last_start_min_sec, convert_to_s=True)\n",
    "        else:\n",
    "            self.win_last_start = None\n",
    "        \n",
    "        # parameters for the windowing procedure\n",
    "        self.win_start_inc = win_start_inc\n",
    "        self.win_stretch_inc = win_stretch_inc\n",
    "        self.win_min_len = win_min_len\n",
    "        self.win_max_len = win_max_len\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.exercise_abbrs = exercise_abbrs\n",
    "        self.exercise_abbrs_peak_eval = exercise_abbrs_peak_eval\n",
    "        self.win_start_min_sec = win_start_min_sec\n",
    "        self.win_last_start_min_sec = win_last_start_min_sec\n",
    "        \n",
    "        # file (csv) of selected test subject\n",
    "        self.test_subject_path = os.path.join(test_subject_dir, test_subject_file)\n",
    "        \n",
    "        # get the feature map, start points, window lengths and the signal length of the selected data\n",
    "        self.feature_map, self.all_window_start_points, \\\n",
    "        self.all_window_lengths, self.signal_len = signal_windowing_via_indices(\n",
    "                                                                         self.test_subject_path,\n",
    "                                                                         number_sections=number_sections,\n",
    "                                                                         sig_names=signal_abbrs,\n",
    "                                                                         signal_orientations=signal_orientations,\n",
    "                                                                         sampling_rate=sampling_rate,\n",
    "                                                                         cutoff=cutoff,\n",
    "                                                                         order=order,\n",
    "                                                                         win_start_inc=win_start_inc,\n",
    "                                                                         win_stretch_inc=win_stretch_inc,\n",
    "                                                                         win_min_len=win_min_len,\n",
    "                                                                         win_max_len=win_max_len,\n",
    "                                                                         win_start=self.win_start,\n",
    "                                                                         win_last_start=self.win_last_start,\n",
    "                                                                         print_progress=print_progress,\n",
    "                                                                         progress_info=progress_info,\n",
    "                                                                         rot_axis=rot_axis,\n",
    "                                                                         rot_angle=rot_angle,\n",
    "                                                                         add_noise=add_noise,\n",
    "                                                                         target_snr_db=target_snr_db,\n",
    "                                                                         csv_skiprows=csv_skiprows,\n",
    "                                                                         csv_separator=csv_separator)\n",
    "        \n",
    "        # last window start time --> time where the minimum window length just fits into the sensor data\n",
    "        self.win_last_start = self.signal_len/self.sampling_rate - self.win_min_len\n",
    "        \n",
    "        # number of different window start points\n",
    "        self.num_start_points = len(self.all_window_start_points)\n",
    "        \n",
    "        # number of different window sizes\n",
    "        self.num_win_sizes = len(self.all_window_lengths)\n",
    "\n",
    "    \n",
    "    # method to get the feature map\n",
    "    def get_feature_map(self):\n",
    "        return self.feature_map\n",
    "    \n",
    "    \n",
    "    def print_rep_blocks(self, print_rep_len_prob=True):\n",
    "        '''\n",
    "        Method to print the found repetition blocks of each exercise \n",
    "        with time range and number of repetitons.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        print_rep_len_prob : boolean\n",
    "            If Ture --> print individual repetition lengths and predicted probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        no returns\n",
    "        '''\n",
    "        \n",
    "        # going through all exercises\n",
    "        for ex in self.exercise_abbrs_peak_eval:\n",
    "            print('\\nExercise: ' + ex)\n",
    "            print('Number of blocks: {}\\n'.format(len(self.rep_blocks[ex])))\n",
    "\n",
    "            # going through all repetition blocks of the current exercise\n",
    "            for block_num in range(len(self.rep_blocks[ex])):\n",
    "                print('\\tBlock #{}:'.format(block_num+1))\n",
    "                print('\\t\\tRepetitions: {}'.format(np.shape(np.array(self.rep_blocks[ex][block_num]))[0]))\n",
    "                \n",
    "                # for both indices we have to consider the start position of the first window (win_start)\n",
    "                start_index = self.rep_blocks[ex][block_num][0,0] + \\\n",
    "                    convert_time_format(self.win_start_min_sec, sampling_rate=1/self.win_start_inc)\n",
    "                stop_index = self.rep_blocks[ex][block_num][-1,0] + \\\n",
    "                    convert_time_format(self.win_start_min_sec, sampling_rate=1/self.win_start_inc)\n",
    "                \n",
    "                # for the stop index we have to consider the length of the last repetition\n",
    "                stop_index += int((self.rep_blocks[ex][block_num][-1,1]*self.win_stretch_inc \\\n",
    "                                   + self.win_min_len) / self.win_start_inc)\n",
    "                \n",
    "                print('\\t\\tTime range: ' + indices_to_time(start_index, stop_index, self.win_start_inc))\n",
    "                \n",
    "                if print_rep_len_prob is True:\n",
    "                    print('\\t\\tRepetition lengths [s] and predicted prob.: ')\n",
    "                    for kk, rep_length_index in enumerate(self.rep_blocks[ex][block_num][:,1]):\n",
    "                        win_pos_index = self.rep_blocks[ex][block_num][kk,0]\n",
    "                        print('\\t\\t\\t{0:3d}\\t{1:.2f}\\t({2:.3f})'.format(kk+1,\n",
    "                                                  rep_length_index*self.win_stretch_inc + self.win_min_len,\n",
    "                                                  self.prob_matrix_dict[ex][win_pos_index,rep_length_index])) \n",
    "    \n",
    "    \n",
    "    def evaluate_probability_matrix(self,\n",
    "                                    pred_probabilities,\n",
    "                                    max_time_between_peaks=10,\n",
    "                                    min_peaks_per_block=3,\n",
    "                                    threshold_prob=0.5,\n",
    "                                    footprint_length=1.5,\n",
    "                                    print_rep_len_prob=True):\n",
    "        '''\n",
    "        Evaluate a probability matrix in order to find repetition blocks.\n",
    "        There are various selectable options --> see Parameters.\n",
    "        After the evaluation the method print_rep_blocks() is called\n",
    "        to print the found repetition blocks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred_probabilities :  np.narray\n",
    "            Matrix with probabilities to evaluate.\n",
    "            \n",
    "        max_time_between_peaks : int or float\n",
    "            Maximum time between two peaks of the same block [s].\n",
    "        \n",
    "        min_peaks_per_block : int\n",
    "            Minimum number of peaks per block.\n",
    "            \n",
    "        threshold_prob : int or float\n",
    "            Find only peaks with a minimum probability (threshold).\n",
    "            (Value from 0 ... 1)\n",
    "        \n",
    "        footprint_length : int or float\n",
    "            Length of the footprint for the maximum_filter in order to find peaks [s].\n",
    "        \n",
    "        print_rep_len_prob : boolean\n",
    "            If Ture --> print individual repetition lengths and predicted probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        no returns\n",
    "        '''\n",
    "        \n",
    "        self.prob_matrix_dict = fill_prediction_matrix(pred_probabilities,\n",
    "                                                       exercise_abbrs=self.exercise_abbrs,\n",
    "                                                       num_start_points=self.num_start_points,\n",
    "                                                       num_win_sizes=self.num_win_sizes,\n",
    "                                                       all_window_start_points=self.all_window_start_points,\n",
    "                                                       all_window_lengths=self.all_window_lengths)\n",
    "\n",
    "        self.peak_ind_dict = {}\n",
    "        for ex in self.exercise_abbrs_peak_eval:\n",
    "            self.peak_ind_dict[ex] = detect_prob_map_peaks(prob_matrix=self.prob_matrix_dict[ex],\n",
    "                                                           win_start_inc=self.win_start_inc,\n",
    "                                                           num_win_sizes=self.num_win_sizes,\n",
    "                                                           threshold_prob=threshold_prob, \n",
    "                                                           footprint_length=footprint_length)\n",
    "\n",
    "        self.rep_blocks = evaluate_peaks(peak_ind=self.peak_ind_dict,\n",
    "                                         prob_matrix=self.prob_matrix_dict,\n",
    "                                         win_start_inc=self.win_start_inc,\n",
    "                                         exercise_abbrs_peak_eval=self.exercise_abbrs_peak_eval,\n",
    "                                         max_time_between_peaks=max_time_between_peaks,\n",
    "                                         min_peaks_per_block=min_peaks_per_block)\n",
    "        \n",
    "        self.print_rep_blocks(print_rep_len_prob)\n",
    "        \n",
    "        \n",
    "    def plot_probability_matrices_and_peaks(self,\n",
    "                                            test_subject_id=None,\n",
    "                                            figsize=(18,9),\n",
    "                                            cross_size=10,\n",
    "                                            plot_actual_classes=True,\n",
    "                                            timetable_file_dir = r'E:\\Physio_Data\\Exercise_time_tables',\n",
    "                                            timetable_file_name = 'Timetable_subject01.txt',\n",
    "                                            exercise_timetable_names = {'Raises Front':'RF',\n",
    "                                                                        'Raises Oblique':'RO',\n",
    "                                                                        'Raises Side':'RS',\n",
    "                                                                        'Rotation Wrist':'LR',\n",
    "                                                                        'Biceps Curls':'BC',\n",
    "                                                                        'Triceps Curls':'TC',\n",
    "                                                                        'Military Press':'MP',\n",
    "                                                                        'Shoulder Adduct.':'SA',\n",
    "                                                                        'PNF Diagonal 1':'P1',\n",
    "                                                                        'PNF Diagonal 2':'P2'}\n",
    "                                           ):\n",
    "        '''\n",
    "        Print the probability matrix as well as the found repetitions\n",
    "        by means of green crosses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_subject_id : int or None\n",
    "            Just for the title of the plot, not necessary.\n",
    "        \n",
    "        figsize : tuple\n",
    "            Figure size of the plot (e.g. (18,9)).\n",
    "            \n",
    "        cross_size : int\n",
    "            Size of the green crosses, indicating the individual repetitions.\n",
    "            \n",
    "        plot_actual_classes : boolean\n",
    "            If True --> show a separate axis with the actual classes from a timetable.\n",
    "        \n",
    "        timetable_file_dir : string\n",
    "            Directory to the timetable file.\n",
    "            (Only necessary if plot_actual_classes is True.)\n",
    "            \n",
    "        timetable_file_name : string\n",
    "            Name of the txt-file containing the timetable with the actual classes.\n",
    "            (Only necessary if plot_actual_classes is True.)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        no returns\n",
    "        '''\n",
    "        \n",
    "        # text for current subject\n",
    "        if isinstance(test_subject_id, int):\n",
    "            self.sub_text = 'Subject {}'.format(test_subject_id)\n",
    "        else:\n",
    "            self.sub_text = ''\n",
    "\n",
    "        yticks = np.arange(0, self.win_max_len-self.win_min_len+self.win_stretch_inc, 2) / self.win_stretch_inc\n",
    "        ylabels = ['{}'.format(yticks[ii] * self.win_stretch_inc + self.win_min_len) for ii in range(len(yticks))]\n",
    "\n",
    "        # plot one axis less if plot_actual_classes is False      \n",
    "        if plot_actual_classes is False:\n",
    "            self.fig, self.axis = plt.subplots(len(self.exercise_abbrs),1,figsize=figsize, sharex=True)\n",
    "        else:\n",
    "            self.fig, self.axis = plt.subplots(len(self.exercise_abbrs)+1,1,figsize=figsize, sharex=True)\n",
    "\n",
    "\n",
    "        # image color settings for RFC probabilities\n",
    "        cmap = plt.cm.seismic\n",
    "        vmin=0\n",
    "        vmax=1\n",
    "\n",
    "        for ax, ex in zip(self.axis, self.exercise_abbrs):\n",
    "            s = ax.imshow(self.prob_matrix_dict[ex].transpose(), interpolation='nearest', \n",
    "                          aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_yticks(yticks)\n",
    "            ax.set_yticklabels(ylabels, fontsize=7)\n",
    "            ax.set_ylabel(ex, rotation=0, fontsize=13)\n",
    "            ax.yaxis.labelpad = 32\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "\n",
    "        # dictionary for cross plots (in order to toggle visibility)\n",
    "        self.cross_plot = {}\n",
    "\n",
    "        # plot crosses for image peaks\n",
    "        for ax, ex in zip(self.axis, self.exercise_abbrs_peak_eval):\n",
    "            #ax.plot(peak_ind[ex][0], peak_ind[ex][1], '+g', markersize=8, markeredgewidth=1.5)\n",
    "            self.cross_plot[ex] = []\n",
    "            for ii in range(len(self.rep_blocks[ex])):\n",
    "                x_peak = np.array(self.rep_blocks[ex][ii])[:,0]\n",
    "                y_peak = np.array(self.rep_blocks[ex][ii])[:,1]\n",
    "                self.cross_plot[ex].append(ax.plot(x_peak, y_peak, '+g', markersize=cross_size, markeredgewidth=1.5))\n",
    "\n",
    "        self.Button_showCross_ax = plt.axes([0.78, 0.12, 0.05, 0.03])\n",
    "        self.Button_showCross = Button(self.Button_showCross_ax, 'Show rep.')\n",
    "        self.Button_showCross.on_clicked(self.toggle_cross)\n",
    "\n",
    "        self.fig.text(0.1, 0.6, r'window length $[s]$', fontsize=10, rotation=90)\n",
    "        # plt.gcf().text(0.078, 0.6, r'window length $[s]$', fontsize=10, rotation=90) # for half the window size\n",
    "\n",
    "        formatter = FuncFormatter(lambda i, x: time.strftime('%M:%S', time.gmtime(i*self.win_start_inc+self.win_start)))\n",
    "        self.axis[-1].xaxis.set_major_formatter(formatter)\n",
    "        self.axis[-1].set_xlabel(r'time $[min:sec]$', fontsize=13)\n",
    "\n",
    "        self.fig.subplots_adjust(bottom=0.2, right=0.9) # make space for buttons and color bar\n",
    "        self.cbar_ax = self.fig.add_axes([0.93, 0.255, 0.01, 0.625])\n",
    "        self.fig.colorbar(s, cax=self.cbar_ax)\n",
    "\n",
    "        # add slider for selections on the x axis\n",
    "        self.Slider_shiftX_ax = plt.axes([0.125, 0.07, 0.775, 0.025])\n",
    "        self.Slider_zoomX_ax = plt.axes([0.125, 0.035, 0.775, 0.025])\n",
    "\n",
    "        axcolor = 'cornflowerblue'\n",
    "        self.Slider_shiftX = Slider(self.Slider_shiftX_ax, 'time shift [%]', 0.0, 100.0, valinit=0, facecolor=axcolor)\n",
    "        self.Slider_zoomX = Slider(self.Slider_zoomX_ax, 'time scale [%]', 0.1, 100.0, valinit=100, facecolor=axcolor)\n",
    "        self.Slider_zoomX_ax.xaxis.set_visible(True)\n",
    "        self.Slider_zoomX_ax.set_xticks(np.arange(0,105,5)) \n",
    "\n",
    "        self.Slider_shiftX.on_changed(self.updateX)\n",
    "        self.Slider_zoomX.on_changed(self.updateX)\n",
    "\n",
    "        # add button to reset view\n",
    "        self.Button_resetX_ax = plt.axes([0.85, 0.12, 0.05, 0.03])\n",
    "        self.Button_resetX = Button(self.Button_resetX_ax, 'Reset view')\n",
    "        self.Button_resetX.on_clicked(self.resetX)\n",
    "\n",
    "        self.start_index = 0\n",
    "        self.stop_index = self.num_start_points\n",
    "\n",
    "        self.fig.suptitle('Predicted Probabilities ' + self.sub_text + '\\n' + indices_to_time(\n",
    "                self.start_index + round(self.win_start/self.win_start_inc),  \n",
    "                self.stop_index + round(self.win_start/self.win_start_inc), \n",
    "                self.win_start_inc), fontsize=20)\n",
    "\n",
    "        self.axis[-1].set_xlim(0, self.num_start_points)\n",
    "\n",
    "\n",
    "        # Plotting the actual classes (exercises) on the last axis:\n",
    "        if plot_actual_classes is True:\n",
    "\n",
    "            # file with timetable (csv) of the test subject\n",
    "            timetable_data_path = os.path.join(timetable_file_dir, timetable_file_name)\n",
    "\n",
    "            # read in time table\n",
    "            timetable_data = pd.read_csv(timetable_data_path, skiprows=0, sep='\\t', header=None)\n",
    "            num_exercises = timetable_data.shape[0] # number of exercises\n",
    "\n",
    "            self.axis[-1].set_yticks([])\n",
    "            self.axis[-1].set_ylim([0,1])\n",
    "\n",
    "            # going through all exercises in the timetable\n",
    "            for ii, ex_name in enumerate(timetable_data.values[:,0]):\n",
    "\n",
    "                # going through all repetition blocks in the timetable (5, 10 and 15 rep. blocks)\n",
    "                for rep_col, start_col, stop_col in zip([1,2,3],[4,6,8],[5,7,9]): # corresponding columns\n",
    "                    rep_num = timetable_data.values[ii,rep_col]\n",
    "                    \n",
    "                    # consider win_start for border calculation\n",
    "                    left_border = convert_time_format(timetable_data.values[ii,start_col], \n",
    "                                                        sampling_rate=1/self.win_start_inc) - \\\n",
    "                                                        self.win_start/self.win_start_inc \n",
    "                    right_border = convert_time_format(timetable_data.values[ii,stop_col], \n",
    "                                                        sampling_rate=1/self.win_start_inc) - \\\n",
    "                                                        self.win_start/self.win_start_inc \n",
    "                    # mark the corresponding area\n",
    "                    self.axis[-1].axvspan(left_border, right_border, color='y', alpha=0.3, lw=0)\n",
    "                    # write text to the corresponding area\n",
    "                    \n",
    "                    # x center of marked area\n",
    "                    x_center = left_border + (right_border-left_border)/2\n",
    "                    self.axis[-1].text(x_center, 0.5, str(rep_num) + '\\n' + exercise_timetable_names[ex_name], \n",
    "                                  horizontalalignment='center', verticalalignment='center', fontsize=10, clip_on=True)\n",
    "\n",
    "            self.axis[-1].set_ylabel('Actual classes', rotation=0, fontsize=11)\n",
    "            self.axis[-1].yaxis.labelpad = 50\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Auxiliary methods for the interactive plot:\n",
    "    \n",
    "    def updateX(self,val):\n",
    "        self.start_index = int(self.Slider_shiftX.val / 100 * self.num_start_points)\n",
    "        self.stop_index = self.start_index + self.Slider_zoomX.val / 100 * self.num_start_points\n",
    "        self.axis[-1].set_xlim((self.start_index, self.stop_index))\n",
    "        self.fig.suptitle('Predicted Probabilities ' + self.sub_text + '\\n' + indices_to_time(\n",
    "            self.start_index + round(self.win_start/self.win_start_inc),  \n",
    "            self.stop_index + round(self.win_start/self.win_start_inc), \n",
    "            self.win_start_inc), fontsize=20)\n",
    "        plt.draw()\n",
    "        \n",
    "    def toggle_cross(self,val):\n",
    "        # This function is called by a button to hide/show the crosses\n",
    "        for ex in self.exercise_abbrs_peak_eval:\n",
    "            for ii in range(len(self.rep_blocks[ex])):\n",
    "                self.cross_plot[ex][ii][0].set_visible(not self.cross_plot[ex][ii][0].get_visible())\n",
    "        plt.draw()\n",
    "        \n",
    "    def resetX(self,val):\n",
    "        self.start_index = 0\n",
    "        self.stop_index = self.num_start_points\n",
    "        self.axis[-1].set_xlim((self.start_index, self.stop_index))\n",
    "        self.Slider_shiftX.reset()\n",
    "        self.Slider_zoomX.reset()\n",
    "        self.fig.suptitle('Predicted Probabilities ' + self.sub_text + '\\n' + indices_to_time(\n",
    "            self.start_index + round(self.win_start/self.win_start_inc),  \n",
    "            self.stop_index + round(self.win_start/self.win_start_inc), \n",
    "            self.win_start_inc), fontsize=20)\n",
    "        plt.draw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T22:49:31.699179Z",
     "start_time": "2019-01-16T22:49:20.446535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate feature map... 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(210945, 60)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD_wp = PhysioData_WindowingProcedure(test_subject_dir  = r'E:\\Physio_Data\\Subject_01',\n",
    "                                      test_subject_file = 'subject01.csv',\n",
    "                                      number_sections=10,\n",
    "                                      signal_abbrs=['Acc','Gyr'],\n",
    "                                      signal_orientations=['x','y','z'],\n",
    "                                      sampling_rate=256,\n",
    "                                      cutoff=10,\n",
    "                                      order=6,\n",
    "                                      win_start_inc=0.2,\n",
    "                                      win_stretch_inc=0.2,\n",
    "                                      win_min_len=1,\n",
    "                                      win_max_len=5,\n",
    "                                      win_start_min_sec='00:00.0',\n",
    "                                      win_last_start_min_sec=None,\n",
    "                                      print_progress=True,\n",
    "                                      progress_info='Generate feature map...',\n",
    "                                      rot_axis=0,\n",
    "                                      rot_angle=0,\n",
    "                                      add_noise=False,\n",
    "                                      target_snr_db=20,\n",
    "                                      csv_skiprows=0,\n",
    "                                      csv_separator=',')\n",
    "\n",
    "np.shape(PD_wp.get_feature_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the created class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:26:40.898094Z",
     "start_time": "2019-01-16T18:26:35.418780Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pivottablejs import pivot_ui\n",
    "import sys\n",
    "sys.path.append('..')  # in order to import modules from my own package\n",
    "\n",
    "# my package\n",
    "from packageMeinhart import PhysioDataHandler as PDH\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_precision_recall_accuracy\n",
    "from packageMeinhart.functionsMasterProjectMeinhart import print_misclassified_data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create and train a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:27:40.439499Z",
     "start_time": "2019-01-16T18:26:40.906094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      "\n",
      "Total Accuracy: 99.01%\n",
      "\n",
      "Exercise\tPrecision [%]\tRecall [%]\tAccuracy [%]\n",
      "  RF\t\t  100.00\t   96.67\t   99.86\n",
      "  RO\t\t   96.77\t  100.00\t   99.86\n",
      "  RS\t\t  100.00\t  100.00\t  100.00\n",
      "  LR\t\t  100.00\t  100.00\t  100.00\n",
      "  BC\t\t  100.00\t  100.00\t  100.00\n",
      "  TC\t\t  100.00\t  100.00\t  100.00\n",
      "  MP\t\t   85.71\t  100.00\t   99.29\n",
      "  SA\t\t  100.00\t  100.00\t  100.00\n",
      "  P1\t\t  100.00\t   96.67\t   99.86\n",
      "  P2\t\t  100.00\t  100.00\t  100.00\n",
      "  NE\t\t   99.75\t   98.77\t   99.15\n",
      "\n",
      "7 misclassified (709 test data points):\n",
      "RF classified as RO\n",
      "P1 classified as NE\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n",
      "NE classified as MP\n"
     ]
    }
   ],
   "source": [
    "PD1 = PDH.PhysioData_SectionFeatures(num_sections=10,\n",
    "                                     test_subject_ids=1,\n",
    "                                     train_subject_ids=-1,\n",
    "                                     test_rep_nums=-1,\n",
    "                                     train_rep_nums=-1,\n",
    "                                     test_ex_abbrs=-1,\n",
    "                                     train_ex_abbrs=-1,\n",
    "                                     with_non_Ex=True,\n",
    "                                     rot_axis_test_data=0,\n",
    "                                     rot_angle_test_data=0,\n",
    "                                     add_noise_test_data=False,\n",
    "                                     add_noise_train_data=False,\n",
    "                                     snr_db=20)\n",
    "\n",
    "# create ML model\n",
    "ML_model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=40, n_jobs=-1, random_state=42)\n",
    "#ML_model = make_pipeline(StandardScaler(), SVC(random_state=42)) # Support Vector Classifier with input scaling\n",
    "\n",
    "# train the model\n",
    "ML_model.fit(PD1.X_train(), PD1.y_train())\n",
    "\n",
    "# predict labels\n",
    "y_pred = ML_model.predict(PD1.X_test())\n",
    "\n",
    "# show results\n",
    "print('Model: ' + type(ML_model).__name__ + '\\n')\n",
    "\n",
    "print('Total Accuracy: {:.2f}%\\n'.format((accuracy_score(PD1.y_test(), y_pred))*100))\n",
    "print_precision_recall_accuracy(y_pred, PD1.y_test())\n",
    "\n",
    "report = classification_report(PD1.y_test(), y_pred, \n",
    "                               labels=np.arange(0,11),\n",
    "                               target_names=['RF','RO','RS','LR','BC','TC','MP','SA','P1','P2','NE'],\n",
    "                               sample_weight=None, output_dict=True)\n",
    "\n",
    "report_df = pd.DataFrame.from_dict(report, orient='index')\n",
    "\n",
    "print('')\n",
    "print_misclassified_data_points(y_pred, PD1.y_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:27:45.090765Z",
     "start_time": "2019-01-16T18:27:40.445500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate feature map... 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40971, 60)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD_wp = PhysioData_WindowingProcedure(test_subject_dir  = r'E:\\Physio_Data\\Subject_01',\n",
    "                                      test_subject_file = 'subject01.csv',\n",
    "                                      number_sections=10,\n",
    "                                      signal_abbrs=['Acc','Gyr'],\n",
    "                                      signal_orientations=['x','y','z'],\n",
    "                                      sampling_rate=256,\n",
    "                                      cutoff=10,\n",
    "                                      order=6,\n",
    "                                      win_start_inc=0.2,\n",
    "                                      win_stretch_inc=0.2,\n",
    "                                      win_min_len=1,\n",
    "                                      win_max_len=5,\n",
    "                                      win_start_min_sec='13:00.0',\n",
    "                                      win_last_start_min_sec='19:30.0',\n",
    "                                      print_progress=True,\n",
    "                                      progress_info='Generate feature map...',\n",
    "                                      rot_axis=0,\n",
    "                                      rot_angle=0,\n",
    "                                      add_noise=False,\n",
    "                                      target_snr_db=20,\n",
    "                                      csv_skiprows=0,\n",
    "                                      csv_separator=',')\n",
    "\n",
    "np.shape(PD_wp.get_feature_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:27:47.835922Z",
     "start_time": "2019-01-16T18:27:45.096766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40971, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs = ML_model.predict_proba(PD_wp.get_feature_map())\n",
    "np.shape(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:27:48.243946Z",
     "start_time": "2019-01-16T18:27:47.842923Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exercise: RF\n",
      "Number of blocks: 3\n",
      "\n",
      "\tBlock #1:\n",
      "\t\tRepetitions: 10\n",
      "\t\tTime range: 13:44.2 - 14:13.8\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t2.80\t(0.716)\n",
      "\t\t\t  2\t3.20\t(0.813)\n",
      "\t\t\t  3\t3.20\t(0.837)\n",
      "\t\t\t  4\t2.80\t(0.673)\n",
      "\t\t\t  5\t2.80\t(0.822)\n",
      "\t\t\t  6\t3.00\t(0.656)\n",
      "\t\t\t  7\t3.00\t(0.619)\n",
      "\t\t\t  8\t3.40\t(0.577)\n",
      "\t\t\t  9\t3.60\t(0.517)\n",
      "\t\t\t 10\t3.60\t(0.621)\n",
      "\tBlock #2:\n",
      "\t\tRepetitions: 15\n",
      "\t\tTime range: 14:39.6 - 15:19.2\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t2.80\t(0.703)\n",
      "\t\t\t  2\t3.20\t(0.784)\n",
      "\t\t\t  3\t2.80\t(0.864)\n",
      "\t\t\t  4\t3.00\t(0.858)\n",
      "\t\t\t  5\t2.80\t(0.812)\n",
      "\t\t\t  6\t2.60\t(0.725)\n",
      "\t\t\t  7\t3.00\t(0.759)\n",
      "\t\t\t  8\t3.00\t(0.823)\n",
      "\t\t\t  9\t2.80\t(0.788)\n",
      "\t\t\t 10\t2.80\t(0.717)\n",
      "\t\t\t 11\t2.60\t(0.802)\n",
      "\t\t\t 12\t2.80\t(0.828)\n",
      "\t\t\t 13\t3.00\t(0.750)\n",
      "\t\t\t 14\t3.00\t(0.723)\n",
      "\t\t\t 15\t2.80\t(0.784)\n",
      "\tBlock #3:\n",
      "\t\tRepetitions: 5\n",
      "\t\tTime range: 16:08.4 - 16:22.6\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t3.00\t(0.729)\n",
      "\t\t\t  2\t3.20\t(0.782)\n",
      "\t\t\t  3\t3.00\t(0.794)\n",
      "\t\t\t  4\t2.80\t(0.649)\n",
      "\t\t\t  5\t3.20\t(0.838)\n",
      "\n",
      "Exercise: RO\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: RS\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: LR\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: BC\n",
      "Number of blocks: 3\n",
      "\n",
      "\tBlock #1:\n",
      "\t\tRepetitions: 15\n",
      "\t\tTime range: 16:49.8 - 17:29.2\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t2.40\t(0.974)\n",
      "\t\t\t  2\t2.60\t(0.987)\n",
      "\t\t\t  3\t2.20\t(0.989)\n",
      "\t\t\t  4\t2.60\t(0.986)\n",
      "\t\t\t  5\t2.20\t(0.987)\n",
      "\t\t\t  6\t2.40\t(0.987)\n",
      "\t\t\t  7\t2.40\t(0.989)\n",
      "\t\t\t  8\t2.80\t(0.986)\n",
      "\t\t\t  9\t2.60\t(0.989)\n",
      "\t\t\t 10\t2.80\t(0.985)\n",
      "\t\t\t 11\t2.40\t(0.985)\n",
      "\t\t\t 12\t3.00\t(0.989)\n",
      "\t\t\t 13\t3.00\t(0.987)\n",
      "\t\t\t 14\t2.80\t(0.986)\n",
      "\t\t\t 15\t2.60\t(0.987)\n",
      "\tBlock #2:\n",
      "\t\tRepetitions: 11\n",
      "\t\tTime range: 18:00.6 - 18:31.4\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t2.40\t(0.987)\n",
      "\t\t\t  2\t2.40\t(0.985)\n",
      "\t\t\t  3\t2.60\t(0.981)\n",
      "\t\t\t  4\t2.60\t(0.988)\n",
      "\t\t\t  5\t2.80\t(0.986)\n",
      "\t\t\t  6\t3.00\t(0.987)\n",
      "\t\t\t  7\t2.40\t(0.989)\n",
      "\t\t\t  8\t2.20\t(0.987)\n",
      "\t\t\t  9\t2.40\t(0.987)\n",
      "\t\t\t 10\t2.80\t(0.986)\n",
      "\t\t\t 11\t2.60\t(0.986)\n",
      "\tBlock #3:\n",
      "\t\tRepetitions: 5\n",
      "\t\tTime range: 18:57.8 - 19:11.0\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t2.20\t(0.985)\n",
      "\t\t\t  2\t2.40\t(0.987)\n",
      "\t\t\t  3\t2.60\t(0.970)\n",
      "\t\t\t  4\t2.80\t(0.988)\n",
      "\t\t\t  5\t2.80\t(0.987)\n",
      "\n",
      "Exercise: TC\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: MP\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: SA\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: P1\n",
      "Number of blocks: 0\n",
      "\n",
      "\n",
      "Exercise: P2\n",
      "Number of blocks: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PD_wp.evaluate_probability_matrix(pred_probabilities=pred_probs,\n",
    "                                  max_time_between_peaks=10,\n",
    "                                  min_peaks_per_block=3,\n",
    "                                  threshold_prob=0.5,\n",
    "                                  footprint_length=1.5,\n",
    "                                  print_rep_len_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:27:49.995046Z",
     "start_time": "2019-01-16T18:27:48.250946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "PD_wp.plot_probability_matrices_and_peaks(test_subject_id=1,\n",
    "                                          figsize=(18,9),\n",
    "                                          cross_size=10,\n",
    "                                          plot_actual_classes=True,\n",
    "                                          timetable_file_dir = r'E:\\Physio_Data\\Exercise_time_tables',\n",
    "                                          timetable_file_name = 'Timetable_subject01.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:28:29.875327Z",
     "start_time": "2019-01-16T18:27:50.070050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=40,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sections=10\n",
    "\n",
    "\n",
    "PD2 = PDH.PhysioData_SectionFeatures(num_sections=num_sections,\n",
    "                                     test_subject_ids=1,\n",
    "                                     train_subject_ids=-1,\n",
    "                                     test_rep_nums=-1,\n",
    "                                     train_rep_nums=-1,\n",
    "                                     test_ex_abbrs=['RF','RO','NE'],\n",
    "                                     train_ex_abbrs=['RF','RO','NE'],\n",
    "                                     with_non_Ex=True,\n",
    "                                     rot_axis_test_data=0,\n",
    "                                     rot_angle_test_data=0,\n",
    "                                     add_noise_test_data=False,\n",
    "                                     add_noise_train_data=False,\n",
    "                                     snr_db=20)\n",
    "\n",
    "# create ML model\n",
    "ML_model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=40, n_jobs=-1, random_state=42)\n",
    "#ML_model = make_pipeline(StandardScaler(), SVC(random_state=42)) # Support Vector Classifier with input scaling\n",
    "\n",
    "# train the model\n",
    "ML_model.fit(PD2.X_train(), PD2.y_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T18:29:54.967194Z",
     "start_time": "2019-01-16T18:29:51.551998Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate feature map... 100%\n",
      "\n",
      "Exercise: RF\n",
      "Number of blocks: 1\n",
      "\n",
      "\tBlock #1:\n",
      "\t\tRepetitions: 10\n",
      "\t\tTime range: 13:44.0 - 14:13.5\n",
      "\t\tRepetition lengths [s] and predicted prob.: \n",
      "\t\t\t  1\t3.00\t(0.542)\n",
      "\t\t\t  2\t3.00\t(0.587)\n",
      "\t\t\t  3\t3.00\t(0.792)\n",
      "\t\t\t  4\t3.00\t(0.694)\n",
      "\t\t\t  5\t2.50\t(0.690)\n",
      "\t\t\t  6\t3.00\t(0.754)\n",
      "\t\t\t  7\t3.00\t(0.860)\n",
      "\t\t\t  8\t3.50\t(0.622)\n",
      "\t\t\t  9\t3.00\t(0.727)\n",
      "\t\t\t 10\t3.00\t(0.893)\n",
      "\n",
      "Exercise: RO\n",
      "Number of blocks: 0\n",
      "\n",
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "PD_wp = PhysioData_WindowingProcedure(test_subject_dir  = r'E:\\Physio_Data\\Subject_01',\n",
    "                                      test_subject_file = 'subject01.csv',\n",
    "                                      number_sections=num_sections,\n",
    "                                      signal_abbrs=['Acc','Gyr'],\n",
    "                                      signal_orientations=['x','y','z'],\n",
    "                                      sampling_rate=256,\n",
    "                                      cutoff=10,\n",
    "                                      order=6,\n",
    "                                      win_start_inc=0.5,\n",
    "                                      win_stretch_inc=0.5,\n",
    "                                      win_min_len=1,\n",
    "                                      win_max_len=4,\n",
    "                                      win_start_min_sec='13:30.0',\n",
    "                                      win_last_start_min_sec='14:30.0',\n",
    "                                      print_progress=True,\n",
    "                                      progress_info='Generate feature map...',\n",
    "                                      rot_axis=[0,2],\n",
    "                                      rot_angle=[10,10],\n",
    "                                      add_noise=True,\n",
    "                                      target_snr_db=10,\n",
    "                                      csv_skiprows=0,\n",
    "                                      csv_separator=',',\n",
    "                                      exercise_abbrs=['RF','RO','NE'],\n",
    "                                      exercise_abbrs_peak_eval = ['RF','RO'])\n",
    "\n",
    "pred_probs = ML_model.predict_proba(PD_wp.get_feature_map())\n",
    "\n",
    "PD_wp.evaluate_probability_matrix(pred_probabilities=pred_probs,\n",
    "                                  max_time_between_peaks=10,\n",
    "                                  min_peaks_per_block=3,\n",
    "                                  threshold_prob=0.5,\n",
    "                                  footprint_length=1.5,\n",
    "                                  print_rep_len_prob=True)\n",
    "\n",
    "%matplotlib auto\n",
    "PD_wp.plot_probability_matrices_and_peaks(test_subject_id=1,\n",
    "                                          figsize=(18,9),\n",
    "                                          cross_size=10,\n",
    "                                          plot_actual_classes=True,\n",
    "                                          timetable_file_dir = r'E:\\Physio_Data\\Exercise_time_tables',\n",
    "                                          timetable_file_name = 'Timetable_subject01.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
